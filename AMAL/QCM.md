* **The double descent phenomenon in deep NNs characterizes a gradient acceleration technique**
    * False
* **Double descent has been characterized for very large neural networks**
    * True: Diapo 108
* **Transposed convolutions are often used for upsampling images**
    * True: RDFIA
* **In ResNets, skip connections have been introduced to improve the stability of classical NNs**
    * True
* **The gating mechanism in GRUs makes use of a form of skip connection**
    * True: Diapo 199
* **The skip gram model is a language model**
    * True: ? 
* **The skip gram model is trained to classify input words**
    * False
* **The skip gram model learns semantic similarities**
    * True
* **The transformers are language models**
    * False: it's a technic used in language model
* **Attention models learn combinations of their inputs**
    * True: Kind of ? (maybe diapo 258)
* **In transformers, self attention is a sequence to sequence operation**
    * True: diapo 262
* **Transformer are encoder-decoder architectures**
    * True?
* **For SVMs, support vectors fully define the decision frontier**
    * True
* **The dual formulation of SVMs is mainly used for linear kernels**
    * False? The dual formulation permit to use any kernels ? 
* **Gaussian processes are trained to predict conditional distributions over functions**
    * True? Diapo 41
* **Gaussian processes are fully defined by a mean function and a covariance function**
    * True: Diapo 41
* **Gaussian processes are Bayesian methods**
    * True? RDFIA + l'introduction faite à partir des modèles Bayesiens
* **Neural processes are kernel methods**
    * False: J'vois pas de lien possible
* **Neural processes allow to predict a mean value and an uncertainty on this mean value**
    * True: diapo 59
* **Neural processes make use of a series of datasets for training**
    * True: those are denoted has tasks