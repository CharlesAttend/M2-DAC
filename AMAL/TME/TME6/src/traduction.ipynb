{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136521/136521 [00:06<00:00, 22234.57it/s]\n",
      "100%|██████████| 34132/34132 [00:01<00:00, 25511.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from icecream import ic\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import time\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILE = \"../data/en-fra.txt\"\n",
    "\n",
    "writer = SummaryWriter(\"/tmp/runs/tag-\" + time.asctime())\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    return re.sub(\n",
    "        \" +\",\n",
    "        \" \",\n",
    "        \"\".join(\n",
    "            c if c in string.ascii_letters else \" \"\n",
    "            for c in unicodedata.normalize(\"NFD\", s.lower().strip())\n",
    "            if c in string.ascii_letters + \" \" + string.punctuation\n",
    "        ),\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "\n",
    "    PAD = 0\n",
    "    EOS = 1\n",
    "    SOS = 2\n",
    "    OOVID = 3\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        self.oov = oov\n",
    "        self.id2word = [\"PAD\", \"EOS\", \"SOS\"]\n",
    "        self.word2id = {\n",
    "            \"PAD\": Vocabulary.PAD,\n",
    "            \"EOS\": Vocabulary.EOS,\n",
    "            \"SOS\": Vocabulary.SOS,\n",
    "        }\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self, idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self, idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "class TradDataset:\n",
    "    def __init__(self, data, vocOrig, vocDest, adding=True, max_len=10):\n",
    "        self.sentences = []\n",
    "        for s in tqdm(data.split(\"\\n\")):\n",
    "            if len(s) < 1:\n",
    "                continue\n",
    "            orig, dest = map(normalize, s.split(\"\\t\")[:2])\n",
    "            if len(orig) > max_len:\n",
    "                continue\n",
    "            self.sentences.append(\n",
    "                (\n",
    "                    torch.tensor(\n",
    "                        [vocOrig.get(o) for o in orig.split(\" \")] + [Vocabulary.EOS]\n",
    "                    ),\n",
    "                    torch.tensor(\n",
    "                        [vocDest.get(o) for o in dest.split(\" \")] + [Vocabulary.EOS]\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    orig, dest = zip(*batch)\n",
    "    o_len = torch.tensor([len(o) for o in orig])\n",
    "    d_len = torch.tensor([len(d) for d in dest])\n",
    "    return pad_sequence(orig), o_len, pad_sequence(dest), d_len\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(FILE) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [lines[x] for x in torch.randperm(len(lines))]\n",
    "idxTrain = int(0.8 * len(lines))\n",
    "\n",
    "vocEng = Vocabulary(True)\n",
    "vocFra = Vocabulary(True)\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datatrain = TradDataset(\"\".join(lines[:idxTrain]), vocEng, vocFra, max_len=MAX_LEN)\n",
    "datatest = TradDataset(\"\".join(lines[idxTrain:]), vocEng, vocFra, max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datatrain, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datatest, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "#  TODO:  Implémenter l'encodeur, le décodeur et la boucle d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mean_train_loss: 3.0872587774551\n",
      "ic| acc_train: 0.5702238082885742\n",
      "ic| mean_test_loss: 2.5962622147787355\n",
      "ic| acc_test: 0.6022915244102478\n",
      " 10%|█         | 1/10 [01:00<09:01, 60.17s/it]ic| mean_train_loss: 2.423137010975452\n",
      "ic| acc_train: 0.622059166431427\n",
      "ic| mean_test_loss: 2.313172186405082\n",
      "ic| acc_test: 0.6374468207359314\n",
      " 20%|██        | 2/10 [01:59<07:55, 59.47s/it]ic| mean_train_loss: 2.223010506032639\n",
      "ic| acc_train: 0.6435925960540771\n",
      "ic| mean_test_loss: 2.1961418707867275\n",
      "ic| acc_test: 0.648361086845398\n",
      " 30%|███       | 3/10 [02:57<06:52, 58.93s/it]ic| mean_train_loss: 2.115829000248194\n",
      "ic| acc_train: 0.6536048054695129\n",
      "ic| mean_test_loss: 2.1287971904980085\n",
      "ic| acc_test: 0.6553044319152832\n",
      " 40%|████      | 4/10 [03:55<05:52, 58.72s/it]ic| mean_train_loss: 2.0478613363194853\n",
      "ic| acc_train: 0.6600785851478577\n",
      "ic| mean_test_loss: 2.0840161432692077\n",
      "ic| acc_test: 0.6606339812278748\n",
      " 50%|█████     | 5/10 [05:05<05:13, 62.67s/it]ic| mean_train_loss: 2.0038991283704886\n",
      "ic| acc_train: 0.6644325256347656\n",
      "ic| mean_test_loss: 2.048591249096461\n",
      "ic| acc_test: 0.6639330387115479\n",
      " 60%|██████    | 6/10 [06:09<04:11, 62.97s/it]ic| mean_train_loss: 1.9675219735432588\n",
      "ic| acc_train: 0.6682107448577881\n",
      "ic| mean_test_loss: 2.021235036022444\n",
      "ic| acc_test: 0.6673367619514465\n",
      " 70%|███████   | 7/10 [07:08<03:05, 61.92s/it]ic| mean_train_loss: 1.9300813867317073\n",
      "ic| acc_train: 0.6723111271858215\n",
      "ic| mean_test_loss: 1.9966663945235634\n",
      "ic| acc_test: 0.6704797744750977\n",
      " 80%|████████  | 8/10 [08:03<01:59, 59.62s/it]ic| mean_train_loss: 1.9034343276984698\n",
      "ic| acc_train: 0.675171434879303\n",
      "ic| mean_test_loss: 1.976319802411278\n",
      "ic| acc_test: 0.6729674339294434\n",
      " 90%|█████████ | 9/10 [08:56<00:57, 57.70s/it]ic| mean_train_loss: 1.8829326947026763\n",
      "ic| acc_train: 0.6772586107254028\n",
      "ic| mean_test_loss: 1.9603608024724206\n",
      "ic| acc_test: 0.674638032913208\n",
      "100%|██████████| 10/10 [09:50<00:00, 59.03s/it]\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, hidden_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    def forward(self, input, input_lengths):\n",
    "        embedded = self.embedding(input)\n",
    "        # packed = pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        # output, h_n = self.gru(packed)\n",
    "        return self.gru(embedded)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_vocab_size, hidden_size, embedding_dim, max_length=MAX_LEN\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.to_vocab = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    def one_step(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Input est soit\n",
    "        * Mode contraint : Les vrais mots de la phrase\n",
    "        * Mode non contraint : Le mot précédent prédit par le décodeur\n",
    "        \"\"\"\n",
    "        output = self.embedding(input)\n",
    "        # ic(output.size())\n",
    "        # output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.to_vocab(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, lens_seq=MAX_LEN):\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        decoder_input = torch.empty(1, batch_size, dtype=torch.long, device=device).fill_(Vocabulary.SOS)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(lens_seq):\n",
    "            # ic()\n",
    "            # ic(decoder_input.size())\n",
    "            # ic(decoder_hidden.size())\n",
    "            decoder_output, decoder_hidden = self.one_step(decoder_input, decoder_hidden)\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                if i == target_tensor.size(0):\n",
    "                    break\n",
    "                decoder_input = target_tensor[i, :].unsqueeze(0) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "            \n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=0)\n",
    "        # decoder_outputs = nn.functional.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden\n",
    "\n",
    "def run_epoch(\n",
    "    loader,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loss_fn,\n",
    "    num_classes,\n",
    "    optimizer=None,\n",
    "    logger=None,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    loss_list = []\n",
    "    acc = tm.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "    acc.to(device)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    if optimizer:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "    if optimizer:\n",
    "        encoder_optimizer, decoder_optimizer = optimizer\n",
    "\n",
    "    for x, len_x, y, len_y in loader:\n",
    "        coin_flip = int(torch.rand(1)) + 1  # stay on teacher forcing mode yet\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Encoder part\n",
    "        encoder_outputs, encoder_hidden = encoder(x, len_x)\n",
    "        \n",
    "        # Decoder part\n",
    "        if coin_flip:  # teacher forcing mode\n",
    "            decoder_outputs, _ = decoder(encoder_outputs, encoder_hidden, y, lens_seq=len_y)\n",
    "        else:\n",
    "            decoder_outputs, _ = decoder(encoder_outputs, encoder_hidden, y, lens_seq=len_y)\n",
    "\n",
    "        # y_oh = nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        # Pour éviter la transformation en OneHot [18, 32, 22904] => [18, 22904, 32] \n",
    "        # Comme ça ça fit le y [18, 32]\n",
    "        decoder_outputs = decoder_outputs.transpose(1,2)\n",
    "        # ic(y.size())\n",
    "        # ic(decoder_outputs.size())\n",
    "        # ic(h_decoder.argmax(-1).size())\n",
    "        loss = loss_fn(decoder_outputs, y)\n",
    "        loss_list.append(loss.item())\n",
    "        acc(decoder_outputs.argmax(1), y)\n",
    "\n",
    "        # backward if we are training\n",
    "        if optimizer:\n",
    "            optimizer[0].zero_grad()\n",
    "            optimizer[1].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer[0].step()\n",
    "            optimizer[1].step()\n",
    "    return np.array(loss_list).mean(), acc.compute().item()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "lr_encoder = lr\n",
    "lr_decoder = lr\n",
    "nb_epoch = 10\n",
    "\n",
    "len_voc_origin = len(vocEng)\n",
    "len_voc_dest = len(vocFra)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = Encoder(len_voc_origin, 16, 64)\n",
    "decoder = Decoder(len_voc_dest, 16, 64)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr_encoder)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "# optimizer.add_param_group(decoder.parameters())\n",
    "for epoch in tqdm(range(nb_epoch)):\n",
    "    mean_train_loss, acc_train = run_epoch(\n",
    "        train_loader,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        loss_fn,\n",
    "        len_voc_dest,\n",
    "        optimizer=(encoder_optimizer, decoder_optimizer),\n",
    "        device=device,\n",
    "    )\n",
    "    mean_test_loss, acc_test = run_epoch(\n",
    "        test_loader, encoder, decoder, loss_fn, len_voc_dest, device=device\n",
    "    )\n",
    "    ic(mean_train_loss)\n",
    "    ic(acc_train)\n",
    "    ic(mean_test_loss)\n",
    "    ic(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
