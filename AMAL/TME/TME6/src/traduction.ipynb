{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136521/136521 [00:05<00:00, 26754.55it/s]\n",
      "100%|██████████| 34132/34132 [00:01<00:00, 21975.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from icecream import ic\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import time\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILE = \"../data/en-fra.txt\"\n",
    "\n",
    "writer = SummaryWriter(\"/tmp/runs/tag-\" + time.asctime())\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    return re.sub(\n",
    "        \" +\",\n",
    "        \" \",\n",
    "        \"\".join(\n",
    "            c if c in string.ascii_letters else \" \"\n",
    "            for c in unicodedata.normalize(\"NFD\", s.lower().strip())\n",
    "            if c in string.ascii_letters + \" \" + string.punctuation\n",
    "        ),\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "\n",
    "    PAD = 0\n",
    "    EOS = 1\n",
    "    SOS = 2\n",
    "    OOVID = 3\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        self.oov = oov\n",
    "        self.id2word = [\"PAD\", \"EOS\", \"SOS\"]\n",
    "        self.word2id = {\n",
    "            \"PAD\": Vocabulary.PAD,\n",
    "            \"EOS\": Vocabulary.EOS,\n",
    "            \"SOS\": Vocabulary.SOS,\n",
    "        }\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self, idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self, idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "class TradDataset:\n",
    "    def __init__(self, data, vocOrig, vocDest, adding=True, max_len=10):\n",
    "        self.sentences = []\n",
    "        for s in tqdm(data.split(\"\\n\")):\n",
    "            if len(s) < 1:\n",
    "                continue\n",
    "            orig, dest = map(normalize, s.split(\"\\t\")[:2])\n",
    "            if len(orig) > max_len:\n",
    "                continue\n",
    "            self.sentences.append(\n",
    "                (\n",
    "                    torch.tensor(\n",
    "                        [vocOrig.get(o) for o in orig.split(\" \")] + [Vocabulary.EOS]\n",
    "                    ),\n",
    "                    torch.tensor(\n",
    "                        [vocDest.get(o) for o in dest.split(\" \")] + [Vocabulary.EOS]\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    orig, dest = zip(*batch)\n",
    "    o_len = torch.tensor([len(o) for o in orig])\n",
    "    d_len = torch.tensor([len(d) for d in dest])\n",
    "    return pad_sequence(orig), o_len, pad_sequence(dest), d_len\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(FILE) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [lines[x] for x in torch.randperm(len(lines))]\n",
    "idxTrain = int(0.8 * len(lines))\n",
    "\n",
    "vocEng = Vocabulary(True)\n",
    "vocFra = Vocabulary(True)\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datatrain = TradDataset(\"\".join(lines[:idxTrain]), vocEng, vocFra, max_len=MAX_LEN)\n",
    "datatest = TradDataset(\"\".join(lines[idxTrain:]), vocEng, vocFra, max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datatrain, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datatest, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "#  TODO:  Implémenter l'encodeur, le décodeur et la boucle d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m# optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m# optimizer.add_param_group(decoder.parameters())\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(nb_epoch)):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     mean_train_loss, acc_train \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m         train_loader,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         encoder,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m         decoder,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m         loss_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m         len_voc_dest,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49m(encoder_optimizer, decoder_optimizer),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     mean_test_loss, acc_test \u001b[39m=\u001b[39m run_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m         test_loader, encoder, decoder, loss_fn, len_voc_dest, device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     ic(mean_train_loss)\n",
      "\u001b[1;32m/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m optimizer[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m optimizer[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m optimizer[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W2sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m optimizer[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, hidden_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    def forward(self, input, input_lengths):\n",
    "        embedded = self.embedding(input)\n",
    "        # packed = pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        # output, h_n = self.gru(packed)\n",
    "        return self.gru(embedded)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_vocab_size, hidden_size, embedding_dim, max_length=MAX_LEN\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.to_vocab = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    def one_step(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Input est soit\n",
    "        * Mode contraint : Les vrais mots de la phrase\n",
    "        * Mode non contraint : Le mot précédent prédit par le décodeur\n",
    "        \"\"\"\n",
    "        output = self.embedding(input)\n",
    "        # ic(output.size())\n",
    "        # output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.to_vocab(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, lens_seq=MAX_LEN):\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        decoder_input = torch.empty(1, batch_size, dtype=torch.long, device=device).fill_(Vocabulary.SOS)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(lens_seq):\n",
    "            # ic()\n",
    "            # ic(decoder_input.size())\n",
    "            # ic(decoder_hidden.size())\n",
    "            decoder_output, decoder_hidden = self.one_step(decoder_input, decoder_hidden)\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                if i == target_tensor.size(0):\n",
    "                    break\n",
    "                decoder_input = target_tensor[i, :].unsqueeze(0) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "            \n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=0)\n",
    "        # decoder_outputs = nn.functional.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden\n",
    "\n",
    "def run_epoch(\n",
    "    loader,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loss_fn,\n",
    "    num_classes,\n",
    "    optimizer=None,\n",
    "    logger=None,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    loss_list = []\n",
    "    acc = tm.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "    acc.to(device)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    if optimizer:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "    if optimizer:\n",
    "        encoder_optimizer, decoder_optimizer = optimizer\n",
    "\n",
    "    for x, len_x, y, len_y in loader:\n",
    "        coin_flip = int(torch.rand(1)) + 1  # stay on teacher forcing mode yet\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Encoder part\n",
    "        encoder_outputs, encoder_hidden = encoder(x, len_x)\n",
    "        \n",
    "        # Decoder part\n",
    "        if coin_flip:  # teacher forcing mode\n",
    "            decoder_outputs, _ = decoder(encoder_outputs, encoder_hidden, y)\n",
    "        else:\n",
    "            decoder_outputs, _ = decoder(encoder_outputs, encoder_hidden, y)\n",
    "\n",
    "        # y_oh = nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        # Pour éviter la transformation en OneHot [18, 32, 22904] => [18, 22904, 32]\n",
    "        decoder_outputs = decoder_outputs.transpose(1,2)\n",
    "        # ic(y.size())\n",
    "        # ic(decoder_outputs.size())\n",
    "        # ic(h_decoder.argmax(-1).size())\n",
    "        loss = loss_fn(decoder_outputs.transpose(0,2), y.transpose(0,1))\n",
    "        loss_list.append(loss.item())\n",
    "        acc(decoder_outputs.argmax(1), y)\n",
    "\n",
    "        # backward if we are training\n",
    "        if optimizer:\n",
    "            optimizer[0].zero_grad()\n",
    "            optimizer[1].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer[0].step()\n",
    "            optimizer[1].step()\n",
    "    return np.array(loss_list).mean(), acc.compute().item()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "lr_encoder = lr\n",
    "lr_decoder = lr\n",
    "nb_epoch = 10\n",
    "\n",
    "len_voc_origin = len(vocEng)\n",
    "len_voc_dest = len(vocFra)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = Encoder(len_voc_origin, 16, 64)\n",
    "decoder = Decoder(len_voc_dest, 16, 64)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr_encoder)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr_decoder)\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "# optimizer.add_param_group(decoder.parameters())\n",
    "for epoch in tqdm(range(nb_epoch)):\n",
    "    mean_train_loss, acc_train = run_epoch(\n",
    "        train_loader,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        loss_fn,\n",
    "        len_voc_dest,\n",
    "        optimizer=(encoder_optimizer, decoder_optimizer),\n",
    "        device=device,\n",
    "    )\n",
    "    mean_test_loss, acc_test = run_epoch(\n",
    "        test_loader, encoder, decoder, loss_fn, len_voc_dest, device=device\n",
    "    )\n",
    "    ic(mean_train_loss)\n",
    "    ic(acc_train)\n",
    "    ic(mean_test_loss)\n",
    "    ic(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m h_encoder \u001b[39m=\u001b[39m encoder(x, len_x)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m generate(h_encoder)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m vec\u001b[39m=\u001b[39mvec\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m all_output \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39;49m(all_output) \u001b[39m<\u001b[39;49m lenseq:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     deb_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(vec))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bppti-14-302-11/tempory/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     _, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(deb_emb, h)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "def generate(h, lenseq=None):\n",
    "    batch_size = h.shape[1]\n",
    "    vec = torch.ones(batch_size)*2\n",
    "    vec = vec.long()\n",
    "    vec=vec.unsqueeze(0)\n",
    "    all_output = []\n",
    "    while len(all_output) < lenseq:\n",
    "        deb_emb = self.relu(self.embedding(vec))\n",
    "        _, h = self.gru(deb_emb, h)\n",
    "        output = self.linear(h)\n",
    "        all_output.append(output)\n",
    "        vec = torch.argmax(output, 2)\n",
    "    return torch.stack(all_output, 2)   \n",
    "\n",
    "for x, len_x, y, _ in train_loader:\n",
    "    x = x.to(device)\n",
    "    h_encoder = encoder(x, len_x)[0]\n",
    "    generate(h_encoder)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 8192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Teacher forcing: Feed the target as the next input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(target_tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# For teacher forcing, the target tensor is passed as the input at the next time step\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     decoder_output, decoder_hidden \u001b[39m=\u001b[39m decoder(target_tensor[t], encoder_hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(decoder_output, target_tensor[t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m output \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(output)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(output, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(output[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/M2-DAC/AMAL/TME/TME6/src/traduction.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:996\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 996\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    999\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:253\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    254\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/M2-DAC/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 8192"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "# ... (autres imports et le code de préparation des données)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    def forward(self, input, input_lengths):\n",
    "        embedded = self.embedding(input)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding_dim, max_length=MAX_LEN):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def generate(self, hidden, lens_seq=None):\n",
    "        inputs = torch.tensor([Vocabulary.SOS]).to(device)\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.max_length if lens_seq is None else lens_seq):\n",
    "            output, hidden = self.forward(inputs, hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            if topi.item() == Vocabulary.EOS:\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(topi.item())\n",
    "                inputs = topi.squeeze().detach()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Encoder and Decoder instantiation\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "encoder = Encoder(len(vocEng), hidden_size, embedding_dim).to(device)\n",
    "decoder = Decoder(hidden_size, len(vocFra), embedding_dim).to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# Assume train_loader is defined and loads (input_tensor, input_length, target_tensor, target_length)\n",
    "for i, (input_tensor, input_length, target_tensor, target_length) in enumerate(train_loader):\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    encoder_hidden = encoder(input_tensor, input_length)\n",
    "\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for t in range(target_tensor.size(0)):\n",
    "        # For teacher forcing, the target tensor is passed as the input at the next time step\n",
    "        decoder_output, decoder_hidden = decoder(target_tensor[t], encoder_hidden)\n",
    "        loss += criterion(decoder_output, target_tensor[t + 1])\n",
    "\n",
    "    # Backpropagation\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "# Save your model\n",
    "torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "\n",
    "# To load the model for testing or further training\n",
    "encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('decoder.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
