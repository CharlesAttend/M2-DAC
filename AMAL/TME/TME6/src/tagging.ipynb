{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tempory/M2-DAC/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:Loading datasets...\n",
      "INFO:root:Vocabulary size: 42932\n",
      "INFO:root:Tags size: 18\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torchmetrics as tm\n",
    "import numpy as np\n",
    "from datamaestro import prepare_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from typing import List\n",
    "import time\n",
    "from icecream import ic\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement s'il n'est pas connu\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "\n",
    "    OOVID = 1\n",
    "    PAD = 0\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        \"\"\"oov : autorise ou non les mots OOV\"\"\"\n",
    "        self.oov = oov\n",
    "        self.id2word = [\"PAD\"]\n",
    "        self.word2id = {\"PAD\": Vocabulary.PAD}\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self, idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self, idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "class TaggingDataset:\n",
    "    def __init__(self, data, words: Vocabulary, tags: Vocabulary, adding=True):\n",
    "        self.sentences = []\n",
    "\n",
    "        for s in data:\n",
    "            self.sentences.append(\n",
    "                (\n",
    "                    [words.get(token[\"form\"], adding) for token in s],\n",
    "                    [tags.get(token[\"upostag\"], adding) for token in s],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    return tuple(\n",
    "        pad_sequence([torch.LongTensor(b[j]) for b in batch]) for j in range(2)\n",
    "    )\n",
    "\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "ds = prepare_dataset(\"org.universaldependencies.french.gsd\")\n",
    "# Format de sortie décrit dans\n",
    "# https://pypi.org/project/conllu/\n",
    "words = Vocabulary(True)\n",
    "tags = Vocabulary(False)\n",
    "train_data = TaggingDataset(ds.train, words, tags, True)\n",
    "dev_data = TaggingDataset(ds.validation, words, tags, True)\n",
    "test_data = TaggingDataset(ds.test, words, tags, False)\n",
    "\n",
    "\n",
    "logging.info(\"Vocabulary size: %d\", len(words))\n",
    "logging.info(\"Tags size: %d\", len(tags))\n",
    "BATCH_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "LEN_WORDS = len(words)\n",
    "LEN_TAG = len(tags)\n",
    "train_loader = DataLoader(\n",
    "    train_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dev_loader = DataLoader(dev_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mean_train_loss: 1.3280937504188148\n",
      "ic| acc_train: 0.6156426668167114\n",
      "ic| mean_test_loss: 0.7863584072031873\n",
      "ic| acc_test: 0.7548145055770874\n",
      " 10%|█         | 1/10 [00:01<00:12,  1.39s/it]ic| mean_train_loss: 0.6630337157871871\n",
      "ic| acc_train: 0.7901821136474609\n",
      "ic| mean_test_loss: 0.572497177631297\n",
      "ic| acc_test: 0.8151452541351318\n",
      " 20%|██        | 2/10 [00:02<00:10,  1.29s/it]ic| mean_train_loss: 0.498495879294598\n",
      "ic| acc_train: 0.841547429561615\n",
      "ic| mean_test_loss: 0.46178415924944777\n",
      "ic| acc_test: 0.851566731929779\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.31s/it]ic| mean_train_loss: 0.39952379006859473\n",
      "ic| acc_train: 0.8747066259384155\n",
      "ic| mean_test_loss: 0.39876654363693076\n",
      "ic| acc_test: 0.8712871074676514\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.28s/it]ic| mean_train_loss: 0.33133375545782323\n",
      "ic| acc_train: 0.8972995281219482\n",
      "ic| mean_test_loss: 0.3487049461679256\n",
      "ic| acc_test: 0.8886682391166687\n",
      " 50%|█████     | 5/10 [00:06<00:06,  1.30s/it]ic| mean_train_loss: 0.27943519371009506\n",
      "ic| acc_train: 0.914116621017456\n",
      "ic| mean_test_loss: 0.316555103723039\n",
      "ic| acc_test: 0.8991132378578186\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.32s/it]ic| mean_train_loss: 0.23854704164957577\n",
      "ic| acc_train: 0.9276311993598938\n",
      "ic| mean_test_loss: 0.29093130662086164\n",
      "ic| acc_test: 0.910727858543396\n",
      " 70%|███████   | 7/10 [00:09<00:03,  1.29s/it]ic| mean_train_loss: 0.20424758534120246\n",
      "ic| acc_train: 0.9389826059341431\n",
      "ic| mean_test_loss: 0.2764840360651625\n",
      "ic| acc_test: 0.9124414920806885\n",
      " 80%|████████  | 8/10 [00:10<00:02,  1.28s/it]ic| mean_train_loss: 0.1755116487301029\n",
      "ic| acc_train: 0.9479758143424988\n",
      "ic| mean_test_loss: 0.26717775806467586\n",
      "ic| acc_test: 0.9166576266288757\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.27s/it]ic| mean_train_loss: 0.15157605079620812\n",
      "ic| acc_train: 0.9557474851608276\n",
      "ic| mean_test_loss: 0.25439348658348654\n",
      "ic| acc_test: 0.9203840494155884\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(\n",
    "    loader, model, loss_fn, optimizer=None, logger=None, device=\"cuda\", num_classes=18\n",
    "):\n",
    "    loss_list = []\n",
    "    acc = tm.classification.Accuracy(\n",
    "        task=\"multiclass\", num_classes=num_classes, ignore_index=Vocabulary.PAD\n",
    "    )\n",
    "    acc.to(device)\n",
    "    model.to(device)\n",
    "    model.train() if optimizer else model.eval()\n",
    "    for input, target in loader:\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # ic(input.size())\n",
    "        # ic(target.size())\n",
    "        output = model(input)\n",
    "        # ic(output.size())\n",
    "        output = model.decode(output).transpose(1, 2)\n",
    "        # ic(output.size())\n",
    "        loss = loss_fn(output, target)\n",
    "        loss_list.append(loss.item())\n",
    "        acc(output.argmax(1), target)\n",
    "        # backward if we are training\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return np.array(loss_list).mean(), acc.compute().item()\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        tag_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.f_h = nn.Linear(hidden_size, tag_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h, (_, _) = self.rnn(x)\n",
    "        return h\n",
    "\n",
    "    def decode(self, h):\n",
    "        return self.f_h(h)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "nb_epoch = 10\n",
    "\n",
    "\n",
    "model = Model(32, 64, LEN_WORDS, LEN_TAG)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=Vocabulary.PAD)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(nb_epoch)):\n",
    "    mean_train_loss, acc_train = run_epoch(\n",
    "        train_loader, model, loss_fn, optimizer, device=device\n",
    "    )\n",
    "    mean_test_loss, acc_test = run_epoch(test_loader, model, loss_fn, device=device)\n",
    "    ic(mean_train_loss)\n",
    "    ic(acc_train)\n",
    "    ic(mean_test_loss)\n",
    "    ic(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
