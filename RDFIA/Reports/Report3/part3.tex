\graphicspath{{figs/3c}}

\chapter{Uncertainty Applications}
% résumé du chapitre


\section{Monte-Carlo Dropout on MNIST}
In this section, we focus on training a model using Monte Carlo Dropout (MC Dropout) on the MNIST dataset. Our primary aim was to identify the most uncertain samples. To do this, we used the variation ratio metric, which effectively measures epistemic uncertainty and is straightforward to calculate. For a given image, denoted as $ \mathbf{x} $, we perform $ T $ stochastic forward passes through the model and record the predicted labels. We then determine the frequency $ f^{c^\star }_\mathbf{x} $ of the most common label ($ c^\star $) across the $ T $ passes. The variation ratio for image $ \mathbf{x} $ is calculated using the formula:
\[
    \text{var-ratio}[\mathbf{x}] = 1 - \frac{f^{c^\star }_\mathbf{x}}{T}
.\]
This formula provides a quantitative measure of uncertainty for an image.


\paragraph*{1.1. What can you say about the images themselfs? How do the histograms along them helps to explain failure cases? Finally, how do probabilities distribution of random images compare to the previous top uncertain images?}

In this experiment, we used a LeNet-5 styled model with Monte-Carlo dropout variational inference. The model was train on MNIST for 20 epoch using cross-entropy in a clasical way. Then we use the model to compute the variation ratios for each test image. This permit to retrieve images by them uncertainty. That what have been done in \Cref{fig:varratio_certain,fig:varratio_certain}, they denote five measurement over the models probabilities outputs for two certain and uncertain images. To sum up the behaviour of the outputed probabilities over the $ T=100 $ stochastic forward pass, we used histograms that display four distribution.
The first columns represent the distribution of the mean of the outputed probability per class.
The second is the distribution of predicted class over the $ T $ forward passes.
Then the 3 last columns represent the distribution of the outputed probability of a particular class (the most predicted class for the 3th column, the ground truth class in the 4th column, a another different class the the 5th column).

Those histogram represent how the output probabilities vary other $ T=100 $ draw. If the model is not confident in its prediction, this will be reflected by high variation in outputed probabilities between each draw and so, histograms will be more sparce and the predicted choosen class will vary.

\Cref{fig:varratio_certain} present images caracterized as certain by the model. With our humain eye, they seem to clearly represent their denoted class. About the presented distributions, they are composed of a single peak, this mean that we approximatly draw the same value every time. The mean probabilities for the predicted class is equal to one and all other class always have a probabilitiy of zero, meaning that the model is pretty confident about it's output. 

In the other hand \Cref{fig:varratio_uncertain} present the same thing for images caracterized as uncertain by the model. Diplayed numbers seem much more ambiguous, and event with our humain eye we have diffuculties to find out which number they represent. 
Distributions are much more spreaded. What does it mean for each columns?Does it mean the same for each columns? In the following part, we will try to explain this behaviour.

For the first two columns, this only mean that many different output class neurons can be activated. To explore more this behaviour we can look at the distribution of the predicted class. As we can see, both histograms are really similar. Cela interroge sur la valeur de la probabilité lorsque la classe est prédite.

The first interpretation that come in mind is that when a class neuron is activated, around his displayed mean value, it is almost the maximum and so the choosen class. 
% j'arrive pas a expliquer, j'ai besoin d'aide en l'expliquant à quelqu'un. Je passe à la question suivante

For the third and fourth column, this mean that they can furthermore take a wide range of values (large std). Those high variation in the output probabilities and in the choosen class translate an unconfident prediction. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{var-ratio_certain_images.pdf}
    \caption{}
    \label{fig:varratio_certain}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{var-ratio_uncertain_images.pdf}
    \caption{}
    \label{fig:varratio_uncertain}
\end{figure}

As we said when looking distribution of the mean of the probabilities of unconfident images, 

\section{Failure prediction}
In this section, we have conducted a comparison of various methods aimed at obtaining a reliable confidence measure for model predictions. Such a measure permit to distinguish correct and incorrect prediction. An intelligent decision system equipped with such metrics can make informed choices, including adhering to the model's prediction or, conversely, involving a human operator, activating a backup system equipped with additional sensors, or triggering an alarm. This field of application is commonly referred to as failure prediction.

During the lecture, we found that Maximum Confidence Probability (MCP) is not a great metric for failure prediction. It assigns high confidence values to both correct and erroneous predictions because modern models tend to be overconfident, resulting in overlapping distributions between successes and errors. This issue persists even when using temperature scaling calibration.

Alternatively, when the model makes a misclassification, the probability associated with the true class $y$ tends to be lower than the maximum probability, often falling to a low value. This observation leads us to consider the True Class Probability (TCP) as a suitable measure of uncertainty. However, the true class labels $y$ are not available when estimating confidence for test inputs. This motivates the development of ConfidNet, whose primary objective is to directly regress the TCP value from the input image, allowing us to obtain a reliable measure of uncertainty without access to ground truth labels.

In this practical, we implemented ConfidNet to address failure predictions and compared it to two other methods that rely solely on the model's output probabilities. The first method is MCP, and the second is the entropy of the output probabilities. For these two methods, we used the previously trained MC Dropout model to compute the output probabilities. ConfidNet was trained for 30 epochs with the previous MC Dropout model as a teacher with frozen parameters and Mean Squared Error as the loss function.

\paragraph*{2.1. Compare the precision-recall curves of each method along with their AUPR values. Why did we use AUPR metric instead of standard AUROC?}
To assess and compare these methods, we require a suitable metric. Our objective is to identify classification errors, which we consider as the positive detection class, while correct predictions serve as the negative detection class. Since our models excel at making accurate predictions, we anticipate a low occurrence of classification errors, resulting in an imbalanced setting with a significant number of true negatives.

We have opted to employ the AUPR (Area Under the Precision-Recall Curve) instead of the AUROC (Area Under the Receiver Operating Characteristic Curve) due to the latter's unsuitability for imbalanced datasets. AUROC treats both classes equally and can yield misleading results, particularly under the influence of a large number of true negatives. This may overstate the model's performance, particularly in distinguishing the minority class, which in this context comprises classification errors. Conversely, AUPR is a more suitable choice, as it prioritizes precision and recall for the minority class. Precision measures the fraction of actual positives among the positive predictions, while recall measures the proportion of correctly identified actual positives. Consequently, AUPR proves to be a more reliable metric in our situation, where the positive class (classification errors) is significantly smaller than the negative class (correct predictions).

\Cref{fig:failure_aupr} illustrates the precision-recall curves for each method, accompanied by their respective AUPR values. The results demonstrate that ConfidNet surpasses the other two methods. This superiority can be attributed to ConfidNet's training, which directly estimates the TCP value—a more dependable uncertainty measure compared to MCP and entropy, as elaborated upon in the preceding section. Notably, ConfidNet exhibits a slower decline in precision as recall increases, indicating a more balanced performance in terms of identifying true positives without a substantial rise in false positives.

For a better understanding of the lecture's confidence metrics, we aimed to compare the performance of predictive entropy and mutual information in the context of failure detection. The results are presented in \Cref{fig:failure_aupr_entropy_vs_mut_inf}. It seems that entropy outperforms mutual information as a metric for failure detection. This distinction arises from the fact that predictive entropy measures aleatoric uncertainty, while mutual information assesses epistemic uncertainty. In the specific experiment of failure detection on MNIST, it proves more advantageous to rely on and measure aleatoric uncertainty, which originates from the natural variability or randomness of the numbers in the images. This explanation underscores why entropy is a superior metric to mutual information in this particular context.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{failure_aupr.pdf}
    \caption{}
    \label{fig:failure_aupr}
\end{figure}

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.45\textwidth]{failure_aupr_entropy_vs_mut_inf.pdf}
    \caption{}
    \label{fig:failure_aupr_entropy_vs_mut_inf}
\end{figure}

\section{Out-of-distribution detection}
Nowadays, models are trained on million of image (like imagenet), making epistemic uncertainty tend to zero. However, critical real word application like autonomous driving, medical imaging, etc. require to detect out-of-distribution (OOD) inputs because of the stochasticness of real life data. In this section, we will use Kuzushiji-MNIST (KMNIST) dataset as out of distribution data for our MC dropout MNIST predictor. This dataset is composed of 70,000 28x28 grayscale number images. We will keep precision, recall and AUPR as metrics to compare the different methods. 

In particular in this section, we implemented the ODIN method \citep{ODIN} that enhance maximum softmax probabilities with temperature scaling and inverse adversarial perturbation. Those two technics are used to increase the difference between in and out of distribution.\\
The temperature scaling is a simple scaling of the logit by a temperature parameter $ \nicefrac{1}{T} $ before the softmax : $ S_i(\boldsymbol{x}, T) = \frac{\exp (f_i(\boldsymbol{x}) / T)}{\sum_{j=1}^{N} \exp (f_j(\boldsymbol{x} / T))} $ with $ S_i $ being the outputed probabilities, $ \boldsymbol{x} $ the input image, $ T $ the temperature parameter and $ f_i $ the logit of the $ i $th class.\\
Inverse adversarial perturbation is to preprocess the input $ \boldsymbol{x} $ before feeding it to the neural network. The preprocessing is done by adding a small perturbation $ \epsilon $ to the input image $ \boldsymbol{x} $ such that $ \boldsymbol{x}^\prime = \boldsymbol{x} - \epsilon \cdot \text{sign}(-\nabla_{\boldsymbol{x}} \mathcal{L}(\boldsymbol{x}, y)) $ with $ \mathcal{L} $ the cross entropy loss function, $ y $ the ground truth label, $ \epsilon $ the perturbation magnitude and $ \boldsymbol{x}^\prime $ the perturbed image. The perturbation magnitude $ \epsilon $ is chosen such that the perturbed image is still classified correctly by the neural network.

\paragraph*{3.1. Compare the precision-recall curves of each OOD method along with their AUPR values. Which method perform best and why?}
\Cref{fig:OOD_aupr} illustrates the precision-recall curves for six uncertainty metrics (MCP, ODIN with a single stochastic passe, CondifNet TCP, ODIN averaged on 100 stochastic forward passes, mutual information and predictive entropy), accompanied by their respective AUPR values. All precision-recall curves are smooth so we can mainly focus on AUPR values for our analysis. As the practical is not clear if we have to disable dropout when computings odin (which is equivalent to use a classical deterministic network), we also tried to compute odin with the mean output over $ T=100 $ forward pass.

A single pass ODIN with his two correction on output probabilities perform slightly better than MCP as expected but not that much. When comparing to the other methods, the single pass ODIN is suprisingly outperformed by all of them. This was intended for CondifNet because of its use of TCP but not for mutual information and predictive entropy that rely on the raw outputed probabilities as ODIN do. In fact, those two last method performed really well with both 98\% of AUPR, standing as the best metrics for OOD detection. 
Even with the leveraging of TCP, ConfidNet is behind with 96\% of AUPR. Mutual information and predictive entropy with their simplicity seem to be the best metrics for OOD detection in our experiments. 
Finally, we can see that ODIN with averaged probability over $ T=100 $ stochastic forward passes give much better result, that are this time near the one of mutual information and predictive entropy. Sugesting that the poor performance of the single pass ODIN is due to the stochasticness of the forward pass. However, ODIN still theoricaly better and should give better result than mutual information and predictive entropy, because of his advanced technics to enhance the difference between in and out of distribution. That still not the case in our experiments.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{OOD_aupr.pdf}
    \caption{}
    \label{fig:OOD_aupr}
\end{figure}

For further inversigation to observe the effect of ODIN technics, we tried to combine it with predictive entropy and mutual information. The results are presented in \Cref{fig:OOD_aupr_combo}. We can see that the combination of ODIN with predictive entropy and mutual information is not better than the metrics alone with a less than 0.10\% augmentation of AUPR. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{OOD_aupr_combo.pdf}
    \caption{}
    \label{fig:OOD_aupr_combo}
\end{figure}

Seeing ODIN still failing, we thought it was because of wrong hyperparameter, in fact, the provided code used $ \epsilon \in {0.006, 0.0006} $ but sugested to use $ \epsilon = 0.0014 $ and the original paper sometime use high value of $ T $, up to $10000$. So we decided to conduct a grid search, AUPR value in fonction of $ \epsilon  $ and $ T $ are presented in \Cref{fig:odin_grid_search} when averaging $ T=100 $ stochastic forward passes. 

The overall effect of those parameters is mild with a difference between the minimum and the maximum score of $ 0.6\% $. Setting the temperature to $ 1 $ is equivalent to diable temperature scaling, this settings allow to find the effect of temperature scaling alone. Thus, the first cell represent a baseline for other cells. We can see by looking at the first line that temperature scaling allow to increase the AUPR by $ 0.6\% $, this is not that much but still a conscient change as we are talking of a effect visible computed over $ 500 $ stochastic forward pass. We can apply the same logic for the effect of perturbation when $ \epsilon = 0 $. This time by looking at the first column, we cannot find any true effect of the perturbation with any value of $ \epsilon $. Thus, combining both parameter lead to the same score as if there was only temperature scaling.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{odin_grid_search.pdf}
    \caption{}
    \label{fig:odin_grid_search}
\end{figure}