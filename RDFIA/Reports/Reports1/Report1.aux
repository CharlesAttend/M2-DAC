\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to neural networks}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Theorical Foundation}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Supervised dataset}{1}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. What are the train, val and test sets used for ?}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. What is the influence of the number of exemples $N$ ?}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Network architecture}{1}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Why is it important to add activation functions between linear transformations?}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. What are the sizes $n_x$, $n_h$, $n_y$ in the figure 1? In practice, how are these sizes chosen?}{1}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. What do the vectors $\hat  {y}$ and $y$ represent? What is the difference between these two quantities?}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Why use a $SoftMax$ function as the output activation function?}{1}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Write the mathematical equations allowing to perform the \textit  {forward} pass of the neural network, i.e. allowing to successively produce $\hat  {h}$, $ h $, $ \tilde  {y} $, $ \hat  {y} $ , starting at x.}{1}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Loss function}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{8. During training, we try to minimize the loss function. For cross entropy and squared error, how must the $ \hat  {y}_i $ vary to decrease the global loss function $ \mathcal  {L} $ ?}{2}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{9. How are these functions better suited to classification or regression tasks?}{2}{section*.9}\protected@file@percent }
\newlabel{CELoss}{{1.2}{2}{9. How are these functions better suited to classification or regression tasks?}{section*.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Optimization algorithm}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{10. What seem to be the advantages and disadvantages of the various variants of gradient descent between the classic, mini-batch stochastic and online stochastic versions? Which one seems the most reasonable to use in the general case?}{2}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{11. What is the influence of the learning rate $ \eta $ on learning?}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{12. Compare the complexity (depending on the number of layers in the network) of calculating the gradients of the loss with respect to the parameters, using the naive approach and the backprop algorithm.}{3}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{13. What criteria must the network architecture meet to allow such an optimization procedure ?}{3}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{14. The function SoftMax and the loss of cross-entropy are often used together and their gradient is very simple. Show that the loss can be simpliÔ¨Åed by:}{3}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{15. Write the gradient of the loss (cross-entropy ) relative to the intermediate output $ \tilde  {y} $ }{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{16. Using the backpropagation, write the gradient of the loss with respect to the weights of the output layer $ \nabla _{W_y}l $ . Note that writing this gradient uses $ \nabla _{\tilde  {y}}l $ . Do the same for $ \nabla _{b_y}l $ .}{4}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{17. Compute other gradients : $ \nabla _{\tilde  {h}} l, \nabla _{W_h} l, \nabla _{b_h}l $ }{4}{section*.17}\protected@file@percent }
\gdef \@abspage@last{4}
