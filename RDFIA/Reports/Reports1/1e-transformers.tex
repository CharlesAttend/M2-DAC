\chapter{Introduction to transformers}
\section{Self Attention}
\paragraph{What is the main feature of self-attention, especially compared to its convolutional counterpart? What is its main challenge in terms of computation/memory?}
Unlike convolutional layers that consider a local neighborhood around each input position, self-attention can weigh all positions across the entire sequence (here the image), providing a form of global receptive field. This is beneficial for tasks where the relevant context can be far from the current position. Convolutional layers have a fixed receptive field, which limits their ability to capture long-range dependencies unless many layers are stacked or dilated convolutions are used. 

The primary challenge of self-attention is its computational and memory complexity, particularly the quadratic complexity with respect to the sequence length. In self-attention, every element in the input sequence interacts with every other element, leading to a time and space complexity of something like $ O(n^2) $, where $n$ is the length of the input sequence. This makes self-attention computationally expensive and memory-intensive for long sequences.

\paragraph{At first, we are going to only consider the simple case of one head. Write the equations and complete the following code. And don't forget a final linear projection at the end!}

First we need to compute three different linear projection of the input $ X $ : the Query $ Q $, the Key $ K $, and the Value $ V $.
\begin{align*}
    Q &= X W_q, \\
    K &= X W_k, \\
    V &= X W_v  \\
\end{align*}


The main part of attention is the dot product between the Query and the Key. Those are a set of vector that we learn. The goal is to learn representation of Key that answer the Query to orient the attention. The dot product is high where the model need attention. 

But we must be aware that when values of the key and query vectors are large, the dot products can become very large. This can result in large values in the softmax function, leading to vanishing gradients during backpropagation. Scaling down the dot products by $ \sqrt[]{d_k} $ (where $ d_k $  is the dimensionality of the query and key vectors) helps in keeping the gradients in a manageable range, which in turn stabilizes the training. As the variance of the dot product grows with the dimensionality, it also keep it constant.

\begin{align*}
    \text{Attention}(Q, K, V) &= \text{Softmax}\left(\frac{QK^t}{\sqrt{d_k}}\right)V \\
\end{align*}

The Value matrix represents the actual content of the input tokens. Once the attention scores are computed, they are used to weight the value vectors $ V $.

\section{Multi-head self-attention}

\paragraph{Write the equations of Multi-Heads Self-Attention.}
\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
.\]
where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$. The Attention function is the same as described in the single-head attention, and $W^O$ is a final linear layer's weights.

\section{Transfomer block}

\paragraph{Write the equations of the transformer block.}
$y = MLP(LayerNorm)$

\section{Full ViT model}

\paragraph{Explain what is a Class token and why we use it?}
The class token is positionned as the first token. It is represented as a learnable vector which gets updated during the forward pass. It accumulate information from different parts of the image as it gets updated through the Transformer layers. As it sum-up all the image, we use it to classify the image.

\paragraph{Explain what is the positional embedding (PE) and why it is important?}
The PE are used to provide the spacial information to the model. Here we use sinusoidal encoding, it provide a fixed, unique and easy to generate encodding for the position. Figure~\ref{fig:pe_viz} is employed to visualize the appearance of sinusoidal encoding. In the context of an image transformer, the x-axis represents the number of image patches created, while the y-axis represents the size of an image patch embedding.  \\
Another approach is to learn this positional embedding. This method allows the model to learn the most useful positional representations. \\
This positionnal embedding is then sum to the image embedding.

\begin{figure}[H]
    \centering
    \includegraphics*[width=.75\textwidth]{figs/Transformers/positional_encoding_subplot.pdf}
    \caption{Visualization of Sinusoidal Positional Encodings in Transformer Models}
    \label{fig:pe_viz}
\end{figure}

\section{Experiment on MNIST!}
The overall performance of our small transformer is really good on MNIST. The convergence is quick and the accuracy really high for every experiments.

\subsubsection{Hyperparameters influences}
As we did in previus experiments, we'll analyse the influence of hyperparmeter by ploting loss and accuracy on the train and test dataset.

\paragraph{Embed dimension}
First let's have a look on the embeded dimension performance in figure~\ref*{fig:embed_dim_influence} where several observations are evident. Firstly, while an embedding dimension of 16 eventually converges, it requires more training time compared to other dimensions. Conversely, an embedding size of 128 shows signs of test loss increasing around the 11th epoch.

As anticipated, a larger embedding dimension can be advantageous but is susceptible to overfitting, as demonstrated by the size 12. Additionally, it is important to note that larger embedding sizes may also necessitate more training time in terms of minutes due to the increased number of parameters.

However, the embedding size of 16 achieves comparable performance to other sizes but with fewer parameters, resulting in reduced energy consumption and training time. This observation aligns with the principle of Occam's razor, emphasizing simplicity as a valuable consideration.

\begin{figure}[H]
    \centering
    \includegraphics*[width=\textwidth]{figs/Transformers/embed_dim_influence.pdf}
    \caption{Influence of the size of the embed dimension}
    \label{fig:embed_dim_influence}
\end{figure}

\paragraph{Patch size}
Figure~\ref*{fig:patch_size_influence} illustrates the impact of patch size on the learning process. Similar to the previous paragraph, it's apparent that all conditions ultimately converge to nearly the same level of accuracy. However, the test loss provides more informative insights.

As anticipated, a patch size as small as 2 by 2 pixels encountered greater challenges in learning due to its diminutive dimensions. On the other hand, a patch size of 28 by 28 pixels is equivalent to the size of an MNIST image, and thus, no patches are formed. This particular configuration, along with a patch size of 14 pixels, exhibits the lowest initial loss and maintains this characteristic throughout most of the training epochs.

\begin{figure}[H]
    \centering
    \includegraphics*[width=\textwidth]{figs/Transformers/patch_size_influence.pdf}
    \caption{Influence of the patch size}
    \label{fig:patch_size_influence}
\end{figure}

\paragraph{Number of transformers blocks}
The influence of the number of transformer blocks is evident in Figure~\ref*{fig:nb_block_influence}. This experiment exhibits similarities to the one conducted with embedding sizes. In all configurations, convergence to a high level of accuracy is observed. Furthermore, increasing the number of transformer blocks reduces the number of epochs required for convergence. However, it's important to note that a high number of blocks can potentially result in overfitting, as indicated by the test loss curve when using 8 blocks, which is notably higher than the other curves.

\begin{figure}[H]
    \centering
    \includegraphics*[width=\textwidth]{figs/Transformers/nb_block_influence.pdf}
    \caption{Influence of the number of transformers blocks}
    \label{fig:nb_block_influence}
\end{figure}

\paragraph{Number of heads in a block}
Once again, the test loss proves to be the most informative aspect of Figure~\ref*{fig:num_heads_influence}, as other aspects appear to converge to similar points. This plot suggests that an increase in the number of self-attention heads tends to yield better results. Additionally, it's noteworthy that the test loss curves do not overlap, which enhances our confidence in the reliability of these results.

\begin{figure}[H]
    \centering
    \includegraphics*[width=\textwidth]{figs/Transformers/num_heads_influence.pdf}
    \caption{Influence of the number of self attention head in one transformer block }
    \label{fig:num_heads_influence}
\end{figure}

\paragraph{MLP hidden layer size}

\begin{figure}[H]
    \centering
    % \includegraphics*[width=\textwidth]{figs/Transformers/mlp_ratio_influence.pdf}
    \caption{Influence of the hidden layer size in the MLP part of a transformer block}
    \label{fig:mlp_ratio_influence}
\end{figure}


\section{Larger transformers}