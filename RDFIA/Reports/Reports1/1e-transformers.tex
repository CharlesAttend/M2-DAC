\chapter{Introduction to transformers}

\begin{figure}[H]
    \centering
    \includegraphics*[width=.8\textwidth]{figs/Transformers/The-Vision-Transformer-architecture-a-the-main-architecture-of-the-model-b-the.png}
\end{figure}

% mettre la source non ?

\section{Self-Attention}
\paragraph{What is the main feature of self-attention, especially compared to its convolutional counterpart? What is its main challenge in terms of computation/memory?}
Unlike convolutional layers that consider a local neighborhood around each input position, self-attention can weigh all positions across the entire sequence (here the image), providing a form of global receptive field. This is beneficial for tasks where the relevant context can be far from the current position. Convolutional layers have a fixed receptive field, which limits their ability to capture long-range dependencies unless many layers are stacked or dilated convolutions are used.

The primary challenge of self-attention is its computational and memory complexity, particularly the quadratic complexity with respect to the sequence length. In self-attention, every element in the input sequence interacts with every other element, leading to a time and space complexity of something like $ O(n^2) $, where $n$ is the length of the input sequence. This makes self-attention computationally expensive and memory-intensive for long sequences.

\paragraph{At first, we are going to only consider the simple case of one head. Write the equations and complete the following code. And don't forget a final linear projection at the end!}

First we need to compute three different linear projection of the input $ X $ : the Query $ Q $, the Key $ K $, and the Value $ V $.

\begin{align*}
    Q & = X W_q, \\
    K & = X W_k, \\
    V & = X W_v
\end{align*}

The main part of attention is the dot product between the Query and the Key. Those are a set of vector that we learn. The goal is to learn representation of Key that answer the Query to orient the attention. The dot product is high where the model need attention.

But we must be aware that when values of the key and query vectors are large, the dot products can become very large. This can result in large values in the softmax function, leading to vanishing gradients during backpropagation. Scaling down the dot products by $ \sqrt[]{d_k} $ (where $ d_k $  is the dimensionality of the query and key vectors) helps in keeping the gradients in a manageable range, which in turn stabilizes the training. As the variance of the dot product grows with the dimensionality, it also keep it constant.

\begin{align*}
    \text{Attention}(Q, K, V) & = \text{Softmax}\left(\frac{QK^t}{\sqrt{d_k}}\right)V
\end{align*}

The Value matrix represents the actual content of the input tokens. Once the attention scores are computed, they are used to weight the value vectors $ V $.

\section{Multi-head self-attention}

\paragraph{Write the equations of Multi-Heads Self-Attention.}
\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
    .\]
where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$. The Attention function is the same as described in the single-head attention, and $W^O$ is a final linear layer's weights.

\section{Transfomer block}

\paragraph{Write the equations of the transformer block.}
Let's define the operations within a Transformers block:

\begin{enumerate}
    \item Let \( x \) be the input to the Transformers block, which in our case is a patch from the image plus a positional embedding. We begin by normalizing the input using Layer Normalization (LayerNorm):

          \[
              z = \text{LayerNorm}(x)
          \]

    \item We then compute the Query (Q), Key (K), and Value (V) matrices, which are used in the Multi-Head Attention mechanism as explained in the previous question:

          \[
              Q = z \cdot W_q, \quad K = z \cdot W_k, \quad V = z \cdot W_v
          \]

    \item Next, we pass these matrices \( Q \), \( K \), and \( V \) into the Multi-Head Attention mechanism:

          \[
              \text{AttOutput} = \text{MultiHead}(Q, K, V)
          \]

    \item We add the normalized input \( z \) to the output of the Multi-Head Attention and apply Layer Normalization again:

          \[
              \text{MidLayerOutput} = \text{LayerNorm}(z + \text{AttOutput})
          \]

    \item We retain this result and add it to the output of the MLP (Multi-Layer Perceptron) head, which is a two-layer linear network with a GeLU (Gaussian Error Linear Unit) or ReLU (Rectified Linear Unit) activation function, defined as follows:

          \[
              \text{MLP}(x) = \text{ReLU}(x \cdot W_1 + b_1) \cdot W_2 + b_2
          \]

    \item Finally, we compute the output of the MLP and add it to the \text{MidLayerOutput} to obtain the final output of the Transformers block:

          \[
              \text{MLPOutput} = \text{MLP}(\text{MidLayerOutput})
          \]
          \[
              \text{FinalOutput} = \text{MLPOutput} + \text{MidLayerOutput}
          \]
          After this last addition, we obtain the final output of a Transformers block.
\end{enumerate}

\section{Full ViT model}

\paragraph{Explain what is a Class token and why we use it?}
The class token is positionned as the first token. It is represented as a learnable vector which gets updated during the forward pass. It accumulate information from different parts of the image as it gets updated through the Transformer layers. As it sum-up all the image, we use it to classify the image.

\paragraph{Explain what is the positional embedding (PE) and why it is important?}
The PE are used to provide the spacial information to the model. Here we use sinusoidal encoding, it provide a fixed, unique and easy to generate encodding for the position. Figure~\ref{fig:pe_viz} is employed to visualize the appearance of sinusoidal encoding. In the context of an image transformer, the x-axis represents the number of image patches created, while the y-axis represents the size of an image patch embedding.  \\
Another approach is to learn this positional embedding. This method allows the model to learn the most useful positional representations. \\
This positionnal embedding is then sum to the image embedding.

\begin{figure}[H]
    \centering
    \includegraphics*[width=.75\textwidth]{figs/Transformers/positional_encoding_subplot.pdf}
    \caption{Visualization of Sinusoidal Positional Encodings in Transformer Models}
    \label{fig:pe_viz}
\end{figure}

\section{Experiment on MNIST!}
The overall performance of our small transformer is really good on MNIST. The convergence is quick and the accuracy really high for every experiments.

\subsection{Hyperparameters influences}
As we did in previus experiments, we'll analyse the influence of hyperparmeter by ploting loss and accuracy on the train and test dataset.

\paragraph{Embed dimension}
First, let's examine the performance of different embedding dimensions in Figure~\ref{fig:embed_dim_influence}. Several notable observations can be made:

In this figure, it's evident that an embedding dimension of 16 takes slightly more epochs to converge compared to other sizes. Conversely, the size of 128 converges at the same speed but achieves the lowest final accuracy. Importantly, this larger size does not exhibit signs of overfitting. It's worth mentioning that training with an embedding size of 128 also requires more time (in terms of minutes) due to the increased number of parameters.

On the other hand, the embedding size of 16 achieves similar performance to the other sizes but with fewer parameters, resulting in reduced energy consumption and training time. This aligns with the principle of Occam's razor, emphasizing the value of simplicity.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/embed_dim_influence_25.pdf}
    \caption{Influence of the size of the embed dimension}
    \label{fig:embed_dim_influence}
\end{figure}

\paragraph{Patch size}
Figure~\ref{fig:patch_size_influence} illustrates the impact of patch size on the learning process. Similar to the previous paragraph, it's apparent that all conditions ultimately converge to nearly the same level of accuracy. However, the test loss provides more informative insights.

As anticipated, a patch size as small as 2 by 2 pixels encountered greater challenges in learning due to its diminutive dimensions. On the other hand, a patch size of 28 by 28 pixels is equivalent to the size of an MNIST image, and thus, no patches are formed. This particular configuration, along with a patch size of 14 pixels, exhibits the lowest initial loss and maintains this characteristic throughout most of the training epochs.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/patch_size_influence.pdf}
    \caption{Influence of the patch size}
    \label{fig:patch_size_influence}
\end{figure}

\paragraph{Number of transformers blocks}
The influence of the number of transformer blocks is evident in Figure~\ref{fig:nb_block_influence}. This experiment exhibits similarities to the one conducted with embedding sizes. In all configurations, convergence to a high level of accuracy is observed. However, it's important to note that a high number of blocks can potentially result in overfitting, as indicated by the test loss curve when using 8 blocks, which is notably higher than the other curves.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/nb_block_influence.pdf}
    \caption{Influence of the number of transformers blocks}
    \label{fig:nb_block_influence}
\end{figure}

\paragraph{Number of heads in a block}
Once again, the test loss proves to be the most informative aspect of Figure~\ref{fig:num_heads_influence}, as other aspects appear to converge to similar points. This plot suggests that an increase in the number of self-attention heads tends to yield better results. Additionally, it's noteworthy that the test loss curves do not overlap, which enhances our confidence in the reliability of these results.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/num_heads_influence.pdf}
    \caption{Influence of the number of self attention head in one transformer block }
    \label{fig:num_heads_influence}
\end{figure}

\paragraph{MLP hidden layer size}
Finally, we conducted tests to assess the influence of the size of the MLP hidden layer, and the results are displayed in Figure~\ref{fig:mlp_ratio_influence}. Analyzing the test loss, show that increasing the hidden size appears to improve the results without inducing overfitting. However, it's important to note that a strong ceiling effect on accuracy limits our ability to draw definitive conclusions.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/mlp_ratio_influence.pdf}
    \caption{Influence of the hidden layer size in the MLP part of a transformer block}
    \label{fig:mlp_ratio_influence}
\end{figure}

\section{Larger transformers}
\subsection{Questions}
\paragraph{What it is the problem and why we have it? Explain if we have also such problem with CNNs.}
The model \texttt{vit\_base\_patch16\_224} is trained on 224x224 images from the ImageNet dataset. In the first layer of the ViT, the input image is divided into fixed-size patches to create embedded vectors. It's crucial to maintain the same image size as the one the model was designed for.

In this case, it necessitates resizing the images to 224x224 RGB pixels. However, this may not be ideal as resizing could distort the images and potentially affect the model's performance.

\paragraph{Provide some ideas on how to make transformer work on small datasets. You can take inspiration from some recent work.}
...

\subsection{Learning from scratch}
As denoted by figure~\ref{fig:stats_vit}, the ViT had difficulty to learn from scratch on MNIST. Due to the model size, four epochs took approximatly 45 minutes and bringed accuracy to somewhat 20 percents on test.

\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/stats_vit.pdf}
    \caption{}
    \label{fig:stats_vit}
\end{figure}


\subsection{Fine-tuning}
Result are much better when fine-tuning as we can see in figure~\ref{fig:stats_vit_pretrained}
\begin{figure}[H]
    \centering
    \includegraphics*[width=0.9\textwidth]{figs/Transformers/stats_vit_pretrained.pdf}
    \caption{}
    \label{fig:stats_vit_pretrained}
\end{figure}

\section{Experiments on Fashion MNIST!}
Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.

\begin{figure}[H]
    \centering
    \includegraphics*[width=.6\textwidth]{figs/Transformers/fashion-mnist-sprite.png}
    \caption{Sample of the Fashion-MNIST dataset (each class takes three-rows)}
\end{figure}

As demonstrated in the previous section, Vision Transformers have pushed the boundaries of the classic MNIST dataset, approaching near-perfect accuracy in testing. To investigate whether a ceiling effect was limiting our ability to draw meaningful conclusions, we replicated all the experiments on the Fashion MNIST dataset. Surprisingly, we did not observe a consistent ceiling effect across all experiments. In some cases, the results were quite similar between the classic MNIST and Fashion MNIST datasets, with the accuracy curves shifting from 100\% to around 80\%. However, in other instances, we gained valuable insights into the impact of various hyperparameters.

We will now briefly review each experiment to see if we can gain new insights. Each figure mentioned will be linked and displayed in the appendix.
\begin{enumerate}
    \item \textbf{Embedding Dimension}: When comparing the results between the classic MNIST dataset (Figure~\ref{fig:embed_dim_influence}) and the Fashion MNIST dataset (Figure~\ref{fig:fashion:embed_dim_influence}), our earlier conclusions drawn from the classic MNIST dataset hold true but are more pronounced. Larger embedding sizes converge at lower accuracy and loss values, while smaller embedding sizes take more time to converge but show promise.

    \item \textbf{Patch Size}: In both the classic MNIST dataset (Figure~\ref{fig:patch_size_influence}) and the Fashion MNIST dataset (Figure~\ref{fig:fashion:patch_size_influence}), patch sizes of 28 or 14 pixels continue to exhibit similar performance.

    \item \textbf{Number of Transformers Blocks}: Our earlier conclusion from the classic MNIST dataset, where a high number of blocks could potentially induce overfitting (Figure~\ref{fig:nb_block_influence}), is not as evident in the Fashion MNIST dataset (Figure~\ref{fig:fashion:nb_block_influence}). Using eight transformers blocks does result in some learning difficulties, as indicated by the red curve having lower accuracy and higher loss.

    \item \textbf{Number of Heads in a Block}: The observation that more self-attention heads tend to yield better results, as seen in the classic MNIST dataset (Figure~\ref{fig:num_heads_influence}), appears to hold true for the Fashion MNIST dataset as well (Figure~\ref{fig:fashion:num_heads_influence}).

    \item \textbf{MLP Hidden Layer Size}: The comparison between the classic MNIST dataset (Figure~\ref{fig:mlp_ratio_influence}) and the Fashion MNIST dataset (Figure~\ref{fig:fashion:mlp_ratio_influence}) reaffirms our previous conclusion. While increasing the MLP hidden layer size may not necessarily induce overfitting, the effect is less clear in the Fashion MNIST dataset.
\end{enumerate}
