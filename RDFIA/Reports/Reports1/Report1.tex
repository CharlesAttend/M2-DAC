\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
% \usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Basics on deep learning for vision}
\author{Charles \textsc{Vin}, Aymeric \textsc{Delefosse}}
\date{S1-2023}

\begin{document}
\maketitle

\section{Introduction to neural networks}
\subsection{Theorical Foundation}
\subsubsection{Supervised dataset}
\paragraph{1. What are the train, val and test sets used for ?}
The train dataset is used to train the model. The test dataset is used to test the model on data it has nether seen before. Finaly the validation set is a separate portion of the dataset used to fine-tune and optimize the model's hyperparameters.

\paragraph{2. What is the influence of the number of exemples $N$ ?}
A large the number of example can help the model to generalize more and be more robust to noise or outlier. A small number of example can prone to overfitting. Increasing N can also increase the computational complexity of training the model.

\subsubsection{Network architecture}
\paragraph{3. Why is it important to add activation functions between linear transformations?}
Otherwise we just sum linear functions so it stays linear. So activation functions introduce non-linearity to the network which permit the model to capture and learn more complex patern than linear. 

\paragraph{4. What are the sizes $n_x$, $n_h$, $n_y$ in the figure 1? In practice, how are these sizes chosen?}
\begin{itemize}
    \item $ n_x = 2 $ is the size of the input, the dimension of our data. 
    \item $ n_h = 4 $ is the size of the hidden layer. It chosen proportionaly to the conplexity of the feature we want to develop in the hiden layer. A large size can lead to overfitting
    \item $ n_y = 2 $  is the size of the output, it's choosen in function of the number of class of $ y $
\end{itemize}
    
\paragraph{5. What do the vectors $\hat{y}$ and $y$ represent? What is the difference between these two quantities?}
$y \in \{0,1\} $ is the ground truth while $ \hat{y} \in [0,1] $ is like a probabilty for each class. $ \hat{y} $ express the model's confidence in each class prediction. 

\paragraph{6. Why use a $SoftMax$ function as the output activation function?}
$ \tilde{y} \in \mathbb{R} $ so we have to transform it into a probability distribution. There is many way to do that but the $SoftMax$ is commonly used in multi-class classification problems.



\paragraph{7. Write the mathematical equations allowing to perform the \textit{forward} pass of the neural network, i.e. allowing to successively produce $\hat{h}$, $ h $, $ \tilde{y} $, $ \hat{y} $  ,  starting at x.}
Let note $ W_i, b_i $ the parameter for the $ i $ layer, $ f_i(x) = W_i x + b_i$ and $ g_i(x) $ the activation function of the layer $ i $.
\begin{align*}
    \tilde{h} &= f_0(x) \\
    h &= g_0(\tilde{h}) \\
    \tilde{y} &= f_1(h) \\
    \hat{y} &= g_1(\tilde{y})
\end{align*}

\subsection{Loss function}
\paragraph{8. During training, we try to minimize the loss function. For cross entropy and squared error, how must the $ \hat{y}_i $  vary to decrease the global loss function $ \mathcal{L} $ ?}
?

\paragraph{9. How are these functions better suited to classification or regression tasks?}
Cross-entropy is better suited classification task for numerus reason : 
\begin{itemize}
    \item Cross-entropy is based on the negative logarithm of the predicted probability of the true class. Due to this logarithmic nature, it magnifies the loss when the predicted probability deviates from 1 (for the correct class) and from 0 (for incorrect classes) as you can see in figure \ref{CELoss}. It would absolutly not work for $ \hat{y} $ outside $ [0,1] $. This amplification of errors is beneficial for training the model effectively. 
    \item Cross-entropy loss is often used in conjunction with the softmax activation function in the output layer of neural networks for multi-class classification. The softmax function ensures that the predicted probabilities sum to 1, a perfect output for the CE-Loss
\end{itemize} 
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=0.6\textwidth]{figs/cross_entropy.png}
    \label{CELoss}
\end{figure}

In the other hand, MSE play the opposite role. It would not work at all with $ \hat{y} \in [0,1] $ but is really good with $ y, \hat{y} \in \mathbb{R} $. It also cool because of it convexity. However, it's essential to note that MSE is not always the best choice for every regression task. For example, ot can be pretty sensitive to outliers.

\subsection{Optimization algorithm}

\paragraph{10. What seem to be the advantages and disadvantages of the various variants of gradient descent between the classic, mini-batch stochastic and online stochastic versions? Which one seems the most reasonable to use in the general case?}

\paragraph{11. What is the influence of the learning rate $ \eta  $  on learning?}
\paragraph{12. Compare the complexity (depending on the number of layers in the network) of calculating the gradients of the loss with respect to the parameters, using the naive approach and the backprop algorithm.}
\paragraph{13. What criteria must the network architecture meet to allow such an optimization procedure ?}
\paragraph{14. The function SoftMax and the loss of cross-entropy are often used together and their gradient is very simple. Show that the loss can be simpliﬁed by:}
\[
    l = - \sum_{i}^{} y_i \tilde{y}_i + \log_{} (\sum_{i}^{} e^{\tilde{y}_i}) 
.\]

\paragraph{15. Write the gradient of the loss (cross-entropy ) relative to the intermediate output $ \tilde{y} $ }
\[
    \frac{\partial l}{\partial \tilde{y}_i}
.\]

\[
    \nabla _{\tilde{y}}l = 
.\]

\paragraph{16. Using the backpropagation, write the gradient of the loss with respect to the weights of the output layer $ \nabla _{W_y}l $ . Note that writing this gradient uses $ \nabla _{\tilde{y}}l $ . Do the same for $ \nabla _{b_y}l $ .}

\paragraph{17. Compute other gradients : $ \nabla _{\tilde{h}} l, \nabla _{W_h} l, \nabla_{b_h}l $ }



\end{document}