\chapter{Visualizing Neural Networks}


\section{Saliency Map}
\paragraph*{$ \bigstar $ Show and interpret the obtained results.}
After visualizing many saliency maps, most of them were perfectly normal but some were pretty inconsistent. In the following, we will describe each case.

% J'interprète souvent comme "le modèle regarde" alors que la saillancy map est que pour la class prédite et pas pour la ground truth faudrait visu les deux mais du coup pour limiter le nombre de figure mettre moins d'example ? 

\Cref{fig:good_saliency_map} show some consistent saliency maps. When the model make a good prediction, we can see high value on the labelled object. It's the case on all image here exept for the fourth one where the model predicted "greenhouse" instead of "lakeside". In fact, with the fourth image, the model did not focused on the lake frontiers to make this wrong prediction. It has mostly looked on the dark ground and the high brithness part of the image leading to the greenhouse class prediction.
\begin{figure}[H]
    \centering
    \includegraphics[width=.95\textwidth]{figs/2b/good_saliency_map.pdf}
    \caption{Consistent saliency maps of the predicted class}
    \label{fig:good_saliency_map}
\end{figure}


\Cref{fig:bad_saliency_map} show some saliency maps that we found inconscistent. Both the first and the fourth one are image where the model prediction is correct but the saliency map is not informative at all, kind of blury. For the second and the last image, we can see that the model is looking at the right place but still predict the wrong class.

\begin{figure}[H]
    \centering  
    \includegraphics[width=.95\textwidth]{figs/2b/bad_saliency_map.pdf}
    \caption{Inconsistent saliency maps of the predicted class}
    \label{fig:bad_saliency_map}
\end{figure}


\paragraph*{Discuss the limits of this technique of visualization the impact of different pixels.}
As we discussed in the previous question, saliency maps can face challenges in scenarios where an image contains multiple objects or complex scenes. In such cases, these maps may become cluttered or ambiguous, making it difficult to extract clear insights. They can also suffer from noise or, conversely, overemphasize certain features while downplaying others. This can result in a skewed interpretation of the model's focus.

Interpreting saliency maps can be subjective when they are ambiguous. Different individuals may draw different conclusions from the same saliency map, leading to inconsistent understandings of the model's behavior.

To gain a better understanding, we found it necessary to repeatedly interpret and visualize various saliency maps, especially when the maps were inconsistent. It can be challenging to keep in mind that each map represents the model's attention for a specific class and not the entire model. Examining saliency maps for other high-probability classes can provide additional context for a more comprehensive understanding of the model's decision.

It's important to note that saliency maps tend to highlight correlation rather than causation. They indicate areas where the model focused its attention but do not necessarily imply that these areas directly caused the model's decision.


\paragraph*{Can this technique be used for a different purpose than interpreting the network ?}
Saliency maps can be used to identify and localize important objects or regions in an image. This can be particularly useful in applications like automated image tagging or initial steps of object detection. This can also  help to create more effective augmented images by applying transformations (like rotations, scaling, cropping) that preserve these key areas.

This technique is predominantly applicable to image data. Its utility is limited in non-visual domains or for models that integrate multiple types of data (like text and images).

%Feature Engineering and Selection: By identifying which parts of an input are most salient to a model's decision, saliency maps can inform feature engineering and selection in machine learning. This can be particularly useful in fine-tuning models by focusing on the most relevant features. ??????

\paragraph*{\textbf{Bonus:} Test with a diffrentent network, for example VGG16, and comment.}
Those are much better! 
% Make a comparaison plot for most of the "bad" saliency map, much better frontier, better classification also, Tibetan mastiff dog much clear frontiere too


\section{Adversarial Example}
\paragraph*{$ \bigstar $ Show and interpret the obtained results.}
\paragraph*{In practice, what consequences can this method have when using convolutional neural networks?}
\paragraph*{\textbf{Bonus:} Discuss the limits of this naive way to construct adversarial images. Can you propose some alternative or modiﬁed ways? (You can base these on recent research)}


\section{Class Visualization}
\paragraph*{$ \bigstar $ Show and interpret the obtained results.}

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        % \includegraphics[width=.4\linewidth]{figs/2b/}
        \caption{Last iteration}
        \label{fig:class_viz_gorilla:png}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        % \includemedia[width=.4\linewidth, height=.4\linewidth]{}{figs/2b/Gorilla_animated_500_regpp_blur_last_frame.gif} % Met tellement de temps à compiler
        \caption{GIF animation (use Adobe Acrobat for viewing or see \textit{Gorilla\_animated\_500\_regpp\_blur.gif})}
        \label{fig:class_viz_gorilla:gif}
    \end{subfigure}

    \caption{Class visualization: started from random noise, maximising the score from the gorilla class}
    \label{fig:class_viz_gorilla}
\end{figure}



\paragraph*{Try to vary the number of iterations and the learning rate as well as the regularization weight.}
Looking at the gif in \Cref*{fig:class_viz_gorilla:gif}, we can see the bluring occuring every 10 steps. 
To make better class visualization, regulation seem to be an important factor. In \Cref*{fig:class_viz_reg}, we tried disabling bluring the image and weight regularization on VGG16. We changed model for obtaining better visuals but all experiments were done on SqueezeNet and VGG16 afterward. Non regulated images seem to be more saturated everywhere, preventing to clearly see the represented class. We suspect that without regularization, the gradient saturate \textbf{everypixel} that could ever had participate to a correct gorilla prediction. By promoting pixel which the model has already engaged the transformation and keeping this direction,regularization permit the arrival of real class image.
% Pas giga clair maybe 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_reg++_last_frame.png}
        \caption{With a lot of regularization}
        \label{fig:class_viz_reg:sub1}
    \end{subfigure}%
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet_bird_animated_last_frame.png}
        \caption{With regularization}
        \label{fig:class_viz_reg:sub2}
    \end{subfigure}%
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet_bird_animated_no_reg_last_frame.png}
        \caption{Without regularization}
        \label{fig:class_viz_reg:sub3}
    \end{subfigure}

    \caption{Comparaison of the bee eater class visualization using VGG16 with or without regularization (blurring and weight regularization) and starting from a base image of the class.}
    \label{fig:class_viz_reg}
\end{figure}

About the number of epoch, our experiment show that we need to make a certain number of epoch to let the visualization converge, thus getting better images.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_1000_regpp_blur_100_frame.png}
        \caption{200 iterations}
        \label{fig:class_viz_iter:sub1}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_1000_regpp_blur_500_frame.png}
        \caption{500 iterations}
        \label{fig:class_viz_iter:sub2}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_1000_regpp_blur_last_frame.png}
        \caption{1000 iterations}
        \label{fig:class_viz_iter:sub3}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_1000_last_frame.png}
        \caption{1000 iterations, no regularization}
        \label{fig:class_viz_iter:sub4}
    \end{subfigure}

    \caption{Same image at different time. The last one is another run but without regularization.}
    \label{fig:class_viz_iter}
\end{figure}

\paragraph*{Try to use an image from ImageNet as the source image instead of a random image (parameter init\_img). You can use the real class as the target class. Comment on the interest of doing this.}

Starting using an image is a good method, it give a good starting point of the gradient to create visualization and not going everywhere like when not using regularization.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        % \includegraphics[width=.4\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_reg++_last_frame.png}
        \caption{Last iteration, starting from noise}
        \label{fig:class_viz_start_image:png_noise}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        % \includegraphics[width=.4\linewidth]{figs/2b/SqueezeNet/SqueezeNet_bird_animated_same_init_img_reg++_last_frame.png}
        \caption{Last iteration, starting from same class}
        \label{fig:class_viz_start_image:png}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        % \includemedia[width=.4\linewidth, height=.4\linewidth]{}{figs/2b/SqueezeNet/SqueezeNet_bird_animated_same_init_img_reg++.gif} % Met tellement de temps à compiler
        \caption{GIF animation, starting from same class (use Adobe Acrobat for viewing or see \textit{SqueezeNet\_bird\_animated\_same\_init\_img\_reg++.gif})}
        \label{fig:class_viz_start_image:gif}
    \end{subfigure}

    \caption{Class visualization: started from a bee eater image to maximising the score for the bee eater class. All with a strong regularization.}
    \label{fig:class_viz_start_image}
\end{figure}


It pretty funny to create some objects from other objects. In \Cref*{fig:class_viz_start_image_dif} we started from a image of hays to do a snail class visualization.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        % \includegraphics[width=.4\linewidth]{figs/2b/SqueezeNet/SqueezeNet_snail_animated_init_img_reg++_last_frame.png}
        \caption{Last iteration}
        \label{fig:class_viz_start_image_dif:png}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        % \includemedia[width=.4\linewidth, height=.4\linewidth]{}{figs/2b/SqueezeNet/SqueezeNet_snail_animated_init_img_reg++.gif} % Met tellement de temps à compiler
        \caption{GIF animation (use Adobe Acrobat for viewing or see \textit{.gif})}
        \label{fig:class_viz_start_image_dif:gif}
    \end{subfigure}

    \caption{Class visualization: started from a hay image to maximising the score for the snail class. All with a strong regularization.}
    \label{fig:class_viz_start_image_dif}
\end{figure}

\paragraph*{\textbf{Bonus:} Test with another network, VGG16, for example, and comment on the results.}
We did the same experiment using VGG16 this time. Visualization are much better because of the overall better performance of VGG16. % Maybe ref la figure saliency map avec VGG16 où il se trompe pas


% Parfois plus délicat de faire des bonne visualization, mais le résultat est clairement plus clair, on voit que le réseau est plus performant que l'autre 