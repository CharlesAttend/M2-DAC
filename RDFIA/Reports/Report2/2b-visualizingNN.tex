\chapter{Visualizing Neural Networks}
\graphicspath{{figs/2b/}}

This exploration focuses on neural network visualization, a crucial aspect of understanding and analyzing how these models make decisions. It emphasizes the importance of interpretability and transparency in machine learning. Through the exploration of various visualization techniques, we can gain valuable insights into how these complex models interpret and process visual information. This endeavor helps bridge the divide between theoretical knowledge and real-world application, contributing to our deeper understanding of how neural networks function and their significance in modern AI solutions.

\section{Saliency Map}

This section focuses on identifying the most impactful pixels in an image for predicting its correct class. It employs a method proposed by \cite{simonyan2014deep}, which approximates the neural network around an image and visualizes the influence of each pixel.

\paragraph*{1. $ \bigstar $ Show and interpret the obtained results.}
In the process of visualizing multiple saliency maps, most of them exhibited consistent patterns, while some displayed inconsistencies. Let's examine each case.

% J'interprète souvent comme "le modèle regarde" alors que la saillancy map est que pour la class prédite et pas pour la ground truth faudrait visu les deux mais du coup pour limiter le nombre de figure mettre moins d'example ? 

In \Cref{fig:good_saliency_map}, we observe saliency maps that are generally consistent. When the model makes accurate predictions, we can observe high saliency values on the labeled object, i.e. it highlights only the important pixels. This makes it easy to recognize the images. This holds true for all the images in this figure, except for the fourth one where the model predicted "greenhouse" instead of "lakeside." In this case, the model did not focus on the lake boundaries, which led to the incorrect prediction. Instead, it primarily concentrated on the dark ground and the bright areas of the image, resulting in the "greenhouse" class prediction. 

In \Cref{fig:bad_saliency_map}, we encounter saliency maps that appear inconsistent. Both the first and fourth images are examples where the model's prediction is correct, but the corresponding saliency maps lack informativeness and appear blurry. In contrast, for the second and last images, the model seems to be focusing on the right regions, but it still predicts the wrong class.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_images}
        \caption{}
        \label{subfig:good_images1}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_true_class_saliency}
        \caption{}
        \label{subfig:good_true_class_saliency}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_predicted_class_saliency}
        \caption{}
        \label{subfig:good_predicted_class_saliency}
    \end{subfigure}
    \caption{Illustration of (a) original images depicting the true label and predicted class by SqueezeNet, (b) saliency maps highlighting regions relevant to the true class, and (c) consistent saliency maps highlighting regions relevant to the predicted class.}
    \label{fig:good_saliency_map}
\end{figure}

%  In (a), the images display both the true label and the class predicted by the SqueezeNet model, providing insight into the model's performance. (b) visualizes regions in the images that contribute to the true class prediction, and (c) visualizes regions contributing to the predicted class, offering insights into the model's decision-making process.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_images}
        \caption{}
        \label{subfig:bad_images}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_true_class_saliency}
        \caption{}
        \label{subfig:bad_true_class_saliency}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_predicted_class_saliency}
        \caption{}
        \label{subfig:bad_predicted_class_saliency}
    \end{subfigure}
    \caption{Illustration of (a) original images depicting the true label and predicted class by SqueezeNet, (b) saliency maps highlighting regions relevant to the true class, and (c) inconsistent saliency maps highlighting regions relevant to the predicted class.}
    \label{fig:bad_saliency_map}
\end{figure}

% figure où le modèle misclassify des images? nan on le fait déjà là

\paragraph*{2. Discuss the limits of this technique of visualization the impact of different pixels.}
As we discussed in the previous question, saliency maps can face challenges when dealing with images containing multiple objects or complex scenes. In such scenarios, these maps may become cluttered or unclear, making it challenging to extract the important pixels. They can also suffer from noise or, conversely, overemphasize certain features while downplaying others. This can result in a biased interpretation of the model's focus. Additionnaly, this technique (Vanilla Gradient) suffers from a saturation problem, as highlihted by \cite{shrikumar2019learning}. When ReLU is used, and when the activation goes below zero, it remains capped at zero and no longer change. This saturation issue may explain our previous observations regarding saliency maps. To address this problem, alternatives methods were subsequently introduced, such as SmoothGrad \citep{smilkov2017smoothgrad} and Grad-CAM \citep{Selvaraju_2019}.

As with most explanation methods, interpreting saliency maps can be subjective, especially when they are ambiguous. Different individuals may draw different conclusions from the same saliency map, resulting in inconsistent interpretations of the model's behavior. This underscores the challenge of determining whether or not an explanation is correct. To gain a better understanding, we found it necessary to repeatedly analyze and visualize various saliency maps, particularly when they exhibited inconsistencies. It can be a complex task to keep in mind that each map represents the model's attention for a specific class, not the entire model. Examining saliency maps for other high-probability classes can provide additional context, contributing to a more complete understanding of the model's decision-making process. 

It's important to note that saliency maps tend to highlight correlations rather than establishing causation. They reveal areas where the model directed its attention but do not necessarily imply that these areas directly influenced the model's decision.

\paragraph*{3. Can this technique be used for a different purpose than interpreting the network?}
Saliency maps can be used to identify and localize important objects or regions in an image. This can be particularly useful in applications like automated image tagging or initial steps of object detection. This can also  help to create more effective augmented images by applying transformations (like rotations, scaling, cropping) that preserve these key areas. This technique is primarily suited for image data. Its utility is limited in non-visual domains or for models that integrate multiple types of data (like text and images).

%Feature Engineering and Selection: By identifying which parts of an input are most salient to a model's decision, saliency maps can inform feature engineering and selection in machine learning. This can be particularly useful in fine-tuning models by focusing on the most relevant features. ??????

\paragraph*{4. \textbf{Bonus:} Test with a diffrentent network, for example VGG16, and comment.} \label{paragraph:bonus_VGG}
% Guillaume a fait sur un ViT aussi
% Make a comparaison plot for most of the "bad" saliency map (ie no more missclassif for example), much better frontier, better classification also, Tibetan mastiff dog much clear frontiere too 
We tried the same saliency maps using VGG16. Those are much better! VGG being better to classify images (no error this time in our examples) than SqueezeNet, this lead to less noise and better object frontier in saliency maps.

In our experimentation with VGG16, we observed notable improvements in the quality of the generated saliency maps compared to those produced with SqueezeNet. We think that VGG16's superior image classification capabilities contributed to reduced noise and more precise object boundaries in the saliency maps.

This outcome highlights the impact of the choice of neural network architecture on the effectiveness of saliency map generation. VGG16's strong performance in image classification results in more reliable and visually coherent saliency maps, making it a favorable choice for tasks that rely on accurate attention mapping within images.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.9\textwidth]{good_saliency_map_vgg16.pdf}
%     \caption{Consistent saliency maps of the predicted class}
%     \label{fig:good_saliency_map_vgg16}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_images_vgg16}
        \caption{}
        \label{subfig:good_images_vgg16}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_true_class_saliency_vgg16}
        \caption{}
        \label{subfig:good_true_class_saliency_vgg16}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{good_predicted_class_saliency_vgg16}
        \caption{}
        \label{subfig:good_predicted_class_saliency_vgg16}
    \end{subfigure}
    \caption{Illustration of (a) original images depicting the true label and predicted class by VGG16, (b) saliency maps highlighting regions relevant to the true class, and (c) consistent saliency maps highlighting regions relevant to the predicted class.}
    \label{fig:good_saliency_map_vgg16}
\end{figure}


% \begin{figure}[H]
%     \centering  
%     \includegraphics[width=.9\textwidth]{bad_saliency_map_vgg16.pdf}
%     \caption{Inconsistent saliency maps of the predicted class}
%     \label{fig:bad_saliency_map_vgg16}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_images_vgg16}
        \caption{}
        \label{subfig:bad_images_vgg16}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_true_class_saliency_vgg16}
        \caption{}
        \label{subfig:bad_true_class_saliency_vgg16}
    \end{subfigure}
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\textwidth]{bad_predicted_class_saliency_vgg16}
        \caption{}
        \label{subfig:bad_predicted_class_saliency_vgg16}
    \end{subfigure}
    \caption{Illustration of (a) original images depicting the true label and predicted class by SqueezeNet, (b) saliency maps highlighting regions relevant to the true class, and (c) consistent saliency maps highlighting regions relevant to the predicted class.}
    \label{fig:bad_saliency_map_vgg16}
\end{figure}


\section{Adversarial Example}

Here, the goal is to study the vulnerability of CNNs to minor, imperceptible modifications in an image that lead to misclassification. This concept, introduced by \cite{szegedy2014intriguing}, reveals the limitations and unexpected behaviors of neural networks. Adversarial examples can be compared to counterfactual examples used in common artificial intelligence explainability methods, but their purpose is to deceive the model rather than interpret it.

\paragraph*{5. $ \bigstar $ Show and interpret the obtained results.}

TODO

% une observation à faire c'est : plus un modèle est sûr de sa prédi, plus il faudra faire d'itérations pour le tromper, et vice-versa.

\paragraph*{6. In practice, what consequences can this method have when using convolutional neural networks?}

For example, consider the ability to trick a model into misclassifying a banana as a toaster using a printable label, as demonstrated by \cite{brown2018adversarial}. While this may seem like a playful experiment, it raises significant concerns when applied to critical domains like autonomous vehicles or medical imaging. 

Although our method requires access to model parameters, \cite{athalye2018synthesizing} has shown that it's possible to perform an adversarial attack without such access, known as a black-box attack. Consequently, an attacker could potentially deceive autonomous vehicles into making hazardous decisions, such as veering into another lane or perceiving non-existent information—incidents that have already occurred. 

This method has the potential to be exploited by malicious individuals to intentionally mislead a model, particularly as machine learning models become increasingly integrated into systems. As a result, this topic becomes a serious concern in the realm of cybersecurity, leading to an arms race between attackers and defenders to safeguard against such vulnerabilities.

\paragraph*{7. \textbf{Bonus:} Discuss the limits of this naive way to construct adversarial images. Can you propose some alternative or modiﬁed ways? (You can base these on recent research)}

% limits ??
One major limit of this naive way is that it requires many pixels to be changed. This led to the question: is it possible to deceive a neural network by modifying only one pixel? \cite{Su_2019} demonstrated that it is actually possible.

This method was inspired by biological evolution studies, and uses differential evolution to determine which to modify and how. It starts with a population of candidate solutions, each representing a potential pixel modification encoded by a five-element vector (coordinates and RGB values). The process then generates new generations of candidates (children) from the existing ones (parents) using a specific formula that involves mixing attributes from three random parent pixels. The process aims to find an adversarial example that causes the classifier to misidentify the image. It continues until such an example is found or until it reaches a user-specified iteration limit.

\section{Class Visualization}

This section aims to generate images that highlight the type of patterns detected by a network for a particular class, based on techniques developed by \cite{simonyan2014deep} and \cite{yosinski2015understanding}. This method helps in visualizing what features the network prioritizes for classification. To view the animated visuals in this section, you can use Adobe Acrobat Reader or access them from a separate folder provided with the materials.

\paragraph*{8. $ \bigstar $ Show and interpret the obtained results.}
Class visualization is a technique aimed at understanding what features or patterns a CNN has learned to recognize for a specific class. It involves generating an input image that maximally activates a class score in the network. Here, we've done this by iteratively modifying an initial random image to increase the activation of the desired class through gradient ascent on the class score with respect to the input image.

As a first try of this method, we decided to visualize our prefered animal: a wombat. \Cref{fig:class_viz_wombat} present a class visualization for a wombat after 1000 epochs and using the default regularization parameters in the given implementation (regularization factor of $10^{-3}$, bluring every ten epoch, learning rate of $5$). 

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.7\linewidth]{SqueezeNet/wombat_animated_1000_last_frame.png}
        \caption{Last iteration}
        \label{fig:class_viz_wombat:png}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        % \begin{frame}{}
        %     \animategraphics[width=.7\linewidth]{10}{SqueezeNet/wombat_animated_1000/frame-}{0}{999}
        % \end{frame}
        % \includemedia[width=.7\linewidth, height=.7\linewidth]{}{SqueezeNet/wombat_animated_1000.mp4}
        \caption{\href{figs/2b/SqueezeNet/wombat_animated_1000.mp4}{Animation}}
        \label{fig:class_viz_wombat:vid}
    \end{subfigure}
    \caption{Class visualization: started from random noise, maximising the score from the wombat class. Using default regularization parameters (regularization factor of $10^{-3}$, bluring every ten epoch, learning rate of $5$).}
    \label{fig:class_viz_wombat}
\end{figure}

\paragraph*{9. Try to vary the number of iterations and the learning rate as well as the regularization weight.}
While examining some of our animations, we observed the occurrence of blurring every 10 steps.

To improve class visualization, it appears that regularization plays a crucial role. In \Cref*{fig:class_viz_reg}, we experimented with disabling image blurring and weight regularization on SqueezeNet.% Although we initially used a different model to achieve better visuals, all experiments were subsequently conducted on both SqueezeNet and VGG16.

% Non regulated images seem to be more saturated everywhere, preventing to clearly see the represented class. We suspect that without regularization, the gradient saturate \textbf{everypixel} that could ever had participate to a correct gorilla prediction. By promoting pixel which the model has already engaged the transformation and keeping this direction,regularization permit the arrival of real class image.

% Pas giga clair maybe 
Non-regularized images appear to be overly saturated, making it challenging to discern the represented class clearly. Our suspicion is that without regularization, gradients become saturated in \textbf{every pixel} that could have contributed to a correct prediction of the ''bee eater'' class. By encouraging pixels that the model has already engaged in transformations and maintaining their direction, regularization facilitates the emergence of the true class image.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_reg++_last_frame.png}
        % \caption{With a lot of regularization}
        \caption{}
        \label{fig:class_viz_reg:sub1}
    \end{subfigure}%
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_last_frame.png}
        % \caption{With regularization}
        \caption{}
        \label{fig:class_viz_reg:sub2}
    \end{subfigure}%
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_no_reg_last_frame.png}
        % \caption{Without regularization}
        \caption{}
        \label{fig:class_viz_reg:sub3}
    \end{subfigure}
    \caption{Comparaison of the bee eater class visualization using SqueezeNet (a) with strong regularizations, (b) with normal regularizations and (c) without regularizations.} % (blurring and weight regularization) % and starting from a base image of the class.
    \label{fig:class_viz_reg}
\end{figure}

% préciser les valeurs de régu ? => Complexe car c'est un combo, un peu chaint à écrire, bluering everimage + 1e-2, ça me semble pas important en vrais

About the number of epoch, our experiment show that we need to make a certain number of epoch to let the visualization converge, thus getting better images.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_1000_regpp_blur_100_frame.png}
        % \caption{200 iterations}
        \caption{}
        \label{fig:class_viz_iter:sub1}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_1000_regpp_blur_500_frame.png}
        % \caption{500 iterations}
        \caption{}
        \label{fig:class_viz_iter:sub2}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_1000_regpp_blur_last_frame.png}
        % \caption{1000 iterations}
        \caption{}
        \label{fig:class_viz_iter:sub3}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_1000_last_frame.png}
        % \caption{1000 iterations, no regularization}
        \caption{}
        \label{fig:class_viz_iter:sub4}
    \end{subfigure}
    \caption{Comparaison of the bee eater class visualization using SqueezeNet with strong regularizations (a) 200 iterations, (b) 500 iterations, (c) 1000 iterations and (d) at 1000 iterations but without having performed regularization.}
    \label{fig:class_viz_iter}
\end{figure}
About the learning rate, TODO

\paragraph*{10. Try to use an image from ImageNet as the source image instead of a random image. You can use the real class as the target class. Comment on the interest of doing this.}

Initiating the process with an image from ImageNet is a valuable approach. It provides a meaningful starting point for gradient-based visualization, ensuring that the initial gradients are directed towards relevant features and patterns within the image. It prevents the initial gradients from scattering or diverging in various directions, potentially resulting in a more focused and interpretable visualization.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_reg++_last_frame.png}
        % \caption{Last iteration\\starting from noise}
        \caption{}
        \label{fig:class_viz_start_image:png_noise}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/bird_animated_same_init_img_reg++_last_frame.png}
        % \caption{Last iteration\\starting from same class}
        \caption{}
        \label{fig:class_viz_start_image:png}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        % \begin{frame}{}
        %     \animategraphics[width=.7\linewidth]{10}{SqueezeNet/bird_animated_same_init_img_reg++/frame-}{0}{199}
        % \end{frame}
        \caption{\href{figs/2b/bird_animated_same_init_img_reg++.gif}{Animation} starting from same class.} %(see \textit{SqueezeNet\_bird\_animated\_same\_init\_img\_reg++.gif})}
        \label{fig:class_viz_start_image:vid}
    \end{subfigure}
    \caption{Class visualization using SqueezeNet: (a) Last iteration starting from noise, (b) Last iteration starting from the same class, and (c) \href{figs/2b/bird_animated_same_init_img_reg++.gif}{Animation} starting from the same class to maximize the score for the bee eater class, all with strong regularization.}
    % \caption{Class visualization: started from a bee eater image to maximising the score for the bee eater class. All with a strong regularization.}
    \label{fig:class_viz_start_image}
\end{figure}

It pretty funny to create some objects from other objects. In \Cref{fig:class_viz_start_image_dif} we started from a image of hays to do a snail class visualization.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.7\linewidth]{SqueezeNet/snail_animated_init_img_reg++_last_frame.png}
        \caption{Last iteration}
        \label{fig:class_viz_start_image_dif:png}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        % \begin{frame}{}
        %     \animategraphics[width=.7\linewidth]{10}{SqueezeNet/snail_animated_init_img_reg++/frame-}{0}{499}
        % \end{frame}
        \caption{\href{figs/2b/SqueezeNet/snail_animated_init_img_reg++.gif}{Animation}}
        \label{fig:class_viz_start_image_dif:vid}
    \end{subfigure}
    \caption{Class visualization using SqueezeNet with strong regularizations: started from a hay image to maximising the score for the snail class.}
    \label{fig:class_viz_start_image_dif}
\end{figure}

\paragraph*{11. \textbf{Bonus:} Test with another network, VGG16, for example, and comment on the results.}
We conducted identical experiments using VGG16 this time. Visualizations are often superior due to VGG16's overall improved performance, as previously discussed in \Cref{paragraph:bonus_VGG}. Specifically, we found the hay to snail animation in \Cref{fig:class_viz_start_image_dif_VGG:vid} to be more appealing. However, there are instances where regularization proves to be more challenging, as observed in \Cref{fig:class_viz_iter_vgg} or \ref{fig:class_viz_wombat_VGG}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.7\linewidth]{VGG/VGG_snail_animated_init_img_reg++_last_frame.png}
        \caption{Last iteration}
        \label{fig:class_viz_start_image_dif_VGG:png}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        % \begin{frame}{}
        %     \animategraphics[width=.7\linewidth]{10}{VGG/VGG_snail_animated_init_img_reg++/frame-}{0}{499}
        % \end{frame}
        \caption{\href{figs/2b/VGG/VGG_snail_animated_init_img_reg++.gif}{Animation}}
        \label{fig:class_viz_start_image_dif_VGG:vid}
    \end{subfigure}
    \caption{Class visualization using VGG with strong regularizations: started from a hay image to maximising the score for the snail class.}
    \label{fig:class_viz_start_image_dif_VGG}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_bird_animated_1000_regpp_blur_100_frame.png}
        % \caption{200 iterations}
        \caption{}
        \label{fig:class_viz_iter_vgg:sub1}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_bird_animated_1000_regpp_blur_500_frame.png}
        % \caption{500 iterations}
        \caption{}
        \label{fig:class_viz_iter_vgg:sub2}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_bird_animated_1000_regpp_blur_last_frame.png}
        % \caption{1000 iterations}
        \caption{}
        \label{fig:class_viz_iter_vgg:sub3}
    \end{subfigure}%
    \begin{subfigure}[t]{.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_bird_animated_1000_last_frame.png}
        % \caption{1000 iterations, no regularization}
        \caption{}
        \label{fig:class_viz_iter_vgg:sub4}
    \end{subfigure}
    \caption{Comparaison of the bee eater class visualization using VGG with strong regularizations (a) 200 iterations, (b) 500 iterations, (c) 1000 iterations and (d) at 1000 iterations but without having performed regularization.}
    \label{fig:class_viz_iter_vgg}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{SqueezeNet/wombat_animated_1000_last_frame.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_wombat_animated_1000_last_frame.png}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VGG/VGG_wombat_animated_1000_regpp_blur_last_frame.png}
        \caption{}
    \end{subfigure}
    \caption{Class visualization of Wombat using VGG TODO}
    \label{fig:class_viz_wombat_VGG}
    %TODO Ici j'trouve les figures plus mitigé qu'avec squeeze net, j'ai envie de mettre celle de squeeze net pour une comparaison plus directe 
\end{figure}

