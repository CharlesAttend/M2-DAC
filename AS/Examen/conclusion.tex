\chapter{conclusion}
Ce tutoriel présente une analyse complète des défis liés à la mise à l'échelle dans les ResNets profonds. Il met en évidence l'importance du facteur de mise à l'échelle \(\alpha_L\), de l'analyse probabiliste et des informations fournies par les modèles en temps continu. En combinant une analyse théorique et des preuves empiriques, on parvient à une compréhension approfondie des mécanismes impliqués dans la formation des ResNets profonds. Cette approche ouvre la voie à la création d'architectures d'apprentissage profond plus efficaces et efficientes à l'avenir.

Premièrement, il convient de mentionner que les Resnets ont été un véritable exploit dans le domaine de l'apprentissage automatique complexe. Ils ont été les premiers modèles de réseaux neuronaux profonds à être entraînés avec succès avec un grand nombre de couches, ce qui a considérablement amélioré les performances. Étant donné son application étendue et son importance, il est essentiel de réfléchir à la manière de construire un cadre "parfait" afin d'éviter tout problème de disparition ou d'explosion de gradient lors de l'entraînement en profondeur, qui pourrait conduire à de mauvais résultats.

Deuxièmement, nous avons constaté, à travers de nombreuses expériences, que la répartition des valeurs initiales (les poids $(V_k)_{1\leqslant k \leqslant L }$ et $(\theta_k)_{1\leqslant k \leqslant L }$)joue un rôle crucial dans les résultats de l'entraînement. Par conséquent, il est impératif d'examiner attentivement la régularité de l'évolution des poids dans le processus de descente de gradient et son impact sur la dynamique de l'entraînement.

La relation entre le taux d'apprentissage et le facteur de mise à l'échelle $\alpha_{L}$ joue un rôle essentiel dans l'évaluation plus précise de la corrélation entre les performances du réseau formé et la mise à l'échelle.