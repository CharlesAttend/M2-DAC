\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Cours}
\author{Charles Vin}
\date{Date}

\begin{document}



\chapter{Introduction}
Avant l'introduction de ResNet en 2015 par He et al, l'architecture GoogLeNet était le dernier gagnant des challenges de vision par ordinateur. Cette architecture venait contrer les problèmes d'apprentissage lié à une augmentation de la profondeur de VGG, une autre architecture proche de ResNet. 

En effet, un réseau plus profond permet apporte dans certaine condition de meilleurs performance, mais aussi des problèmes comme l'explosion ou l'évanouissement du gradient de la loss. Lors de la backpropagation, les grandes ou petites valeurs peuvent s'amplifier à chaque couche du réseau donnant un gradient beaucoup plus grand/petit à la dernière couche du réseau en comparaison de la première. C'est un effet multiplicatif et donc en lien avec la profondeur du réseau.

Pour un réseau d'une profondeur $ L $, on modélise ces états cachés de dimension $ d $  par une séquence $ (h_k)_{1 \leq k \leq L} $ avec $ h_k \in \mathbb{R}^d, \forall 0 \leq k \leq L $. On peut décrire mathématiquement l'explosion du gradient tel que, avec une forte probabilité, $ \left\| \frac{\partial \mathcal{L}}{\partial h_0} \right\|  \gg  \left\| \frac{\partial \mathcal{L}}{\partial h_L} \right\|  $ où $ \mathcal{L} $ représente la loss et $ \left\| \cdot  \right\|  $ la norme euclidienne.


La solution apporté par GoogLeNet n'améliorait pas tant les performances comparé à VGG et restait assez complexe. Sa profondeur était comparable à celle de VGG, passant de 22 à 16 couches. En 2015, ResNet arriva avec un modèle allant jusqu'à 152 couches, divisant par deux le nombre d'erreur de GoogLeNet. Son inovation est la présence de \textit{skip connections} entre les couches successives, permetant un meilleurs passage du gradient au sein du réseau. Mathématiquement, on peut alors écrire la relation récurente pour la suite $ (h_k)_{1 \leq k \leq L} $ tel que 
\[
    h_{k+1 } = h_k + f(h_k, \theta _{k+1})
.\]
avec $ f(\cdot , \theta _{k+1}) $ représente plusieurs transformation faites par la couche $ k $ en question et paramétrisé par $ \theta _{k+1} \in \mathbb{R}^p $.

ResNets est devenus la base de nombreux modèles d'apprentissage profond de pointe, dépassant le traitement d'images pour s'étendre à divers domaines tels que le traitement du langage naturel et l'apprentissage par renforcement. L'idée des \textit{skip connections} a inspiré de nombreuses autres architectures et est devenue une pratique standard dans la conception des réseaux neuronaux profonds.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=.65\textwidth]{name}
    \caption{Illustration du modèle ResNet. Notons la présence de la \textit{skip connection} au sein de chaque bloc.}
    \label{fig:resnet}
\end{figure}

Malgrès cette technique, ResNet soufre toujours de problème de gradient lors de l'apprentissage. L'approche historique pour éviter cela est de normaliser les état cachés à la sortie de chaque couche (\textit{batch normalization}). Toutefois, cela à un cout computationnel et une forte dépendance par rapport à la taille du \textit{batch}. Une alternative consiste à incorporer un facteur d'échelle $ \alpha _L $ devant le terme résiduel, ce qui conduit au modèle suivant : 
\[
    h_{k+1} = h_k + \alpha_L f(h_k, \theta_{k+1})
.\]
Le choix de ce facteur $ \alpha _L $ est cruciale et dépend naturellement de la profondeur du réseau. Il garantit que la variance du signal ne change pas radicalement lorsqu'il se propage à travers les couche. Mais il n'y a pour l'instant pas de preuve formel ni de preuve mathématique dans le choix de ce facteur de régularisation.

% LIEN AVEC LES ODE A INTRODUIRE ICI ?

Dans ce cours, nous étudirons les fondements mathématiques du pourquoi et comment choisir la valeur de $ \alpha _L $ en fonction de $ L $  et de la distribution initiale des poids afin d'éviter les problèmes d'apprentissage. Pour cela, il existe deux grands axes d'étude :
\begin{enumerate}
    \item $ \alpha _L  $ à l'initialisation : L'initialition des paramètres joue un rôle majeur dans le bon déroulement de la phase d'apprentissage d'un modèle et même au dela sur les capacités de généralisation d'un modèle. Une mauvaise initialisation peut entrainer une divergence ou un évanouissement rapide du gradient, voir même un blocage dans l'apprentissage. Etudier le role de $ \alpha _L  $ au moment de l'initialisation est intéréssant. Nous nous placerons donc dans le contexte mathématique suivant : a l'initialisation les poids de chaque couche $ (\theta _k)_{1 \leq k \leq L} $ sont choisis comme la réalisation indépendament et identiquement distribué (i.i.d) d'une loi, classiquement Gaussienne ou uniforme sur $ \mathbb{R}^p $.
    \item L'approche continue 
\end{enumerate}


\chapter*{$ \alpha _L  $ à l'initialisation}
























For any $ 1 \leq  k \leq L  $ 

\paragraph*{Assumptions 1}
For some $ s \geq 1 $, the entries of $ \sqrt[]{d}V_k $ are symmetric i.i.d., $ s^2 $ sub-Gaussian random variable, independent of $ d $ and $ L $, with unit variance. 

\paragraph*{Assumptions 2}
For some $ C > 0 $, independent of $ d $ and $ L $, and for any $ h \in \mathbb{R}^D  $ 
\[
    \frac{\left\| h \right\| ^2}{2 } \leq  \mathbb{E }[ \left\|  g(h, \theta _ k ) \right\| ^2 ] \leq \left\| h \right\| ^2
.\]
\[
    \mathbb{E } [\left\| g(h, \theta _k)  \right\| ^8 ]\leq C \left\| h  \right\| ^8
.\]

\paragraph*{Proposition 2}[Admited ?]  Consider a ResNet (4) such that Assumptions (A1) and (A2) are satisfied.
If \( L\alpha_L^2 \leq 1 \), then, for any \( \delta \in (0, 1) \), with probability at least \( 1 - \delta \),
\[
\frac{\|h_L - h_0\|^2}{\|h_0\|^2} \leq \frac{2L\alpha_L^2}{\delta}.
\]

\paragraph*{Proposition 3}[Admited] Consider a ResNet (4) such that Assumptions (A1) and (A2) are satisfied.
\begin{itemize}
    \item[(i)] Assume that $d \geq 64$ and $ \alpha _L ^2 \leq  \frac{2 }{(\sqrt[]{C } s^4 + 4 \sqrt[]{C } + 16 s ^4)d}$. Then, for any $\delta \in (0, 1)$, with probability at least $1 - \delta$,
    \[
    \frac{\|h_L - h_0\|^2}{\|h_0\|^2} > \exp\left(\frac{3L\alpha_L^2}{8} - \sqrt{\frac{11L\alpha_L^2}{d\delta}}\right) - 1,
    \]
    provided that
    \[
    2L \exp\left(-\frac{d}{64\alpha_L^2s^2}\right) \leq \frac{\delta}{11}.
    \]

    \item[(ii)] Assume that $\alpha_L^2 \leq \frac{1}{\sqrt{C}(d + 128s^4)}$. Then, for any $\delta \in (0, 1)$, with probability at least $1 - \delta$,
    \[
    \frac{\|h_L - h_0\|^2}{\|h_0\|^2} < \exp\left(L\alpha_L^2 + \sqrt{\frac{5L\alpha_L^2}{d\delta}}\right) + 1.
    \]
\end{itemize}

\begin{cor}[4]
    Consider a ResNet (4) such that Assumptions (A1) and (A2) are satisfied, and let $\alpha_L = 1/L^\beta$, with $\beta > 0$.
\begin{itemize}
    \item[(i)] If $\beta > \frac{1}{2}$, then
    \[
    \frac{\|h_L - h_0\|}{\|h_0\|} \xrightarrow{\mathbb{P}} 0 \text{ as } L \to \infty.
    \]
    \item[(ii)] If $\beta < \frac{1}{2}$ and $d \geq 9$, then
    \[
    \frac{\|h_L - h_0\|}{\|h_0\|} \xrightarrow{\mathbb{P}} \infty \text{ as } L \to \infty.
    \]
    \item[(iii)] If $\beta = \frac{1}{2}$, $d \geq 64$, $L \geq \left(\frac{1}{2}\sqrt{C}s^4 + 2\sqrt{C} + 8s^4)d + 96\sqrt{C}s^4\right)$, then, for any $\delta \in (0, 1)$, with probability at least $1 - \delta$,
    \[
    \exp\left(\frac{3}{8} - \sqrt{\frac{22}{d\delta}}\right) - 1 < \frac{\|h_L - h_0\|^2}{\|h_0\|^2} < \exp\left(1 + \sqrt{\frac{10}{d\delta}}\right) + 1,
    \]
    provided that
    \[
    2L \exp\left(-\frac{Ld}{64s^2}\right) \leq \frac{\delta}{11}.
    \]
\end{itemize}

\end{cor}
\begin{proof}[Proof: ]
    Statement ($ i $) is a consequence of Proposition 2. We have $ L \alpha _L ^2 = \frac{L}{L^{2\beta} } = L^{1 - 2 \beta } $, as $ \beta > 1/2 \Leftrightarrow 1 - 2 \beta < 0$ we have $L^{1 - 2 \beta } = \frac{1}{L^{2 \beta  -1}} \underset{L\to +\infty}{\longrightarrow} 0 $. Thus
    \begin{align*}
        & \frac{\|h_L - h_0\|^2}{\|h_0\|^2} \leq \frac{2L\alpha_L^2}{\delta}.
        \overunderset{\mathbb{P}}{L\to +\infty}{\longrightarrow} 0 
    \end{align*}

    Statement ($ ii $) is a consequence of Proposition 3.


\end{proof}






\end{document}