\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Cours}
\author{Charles Vin}
\date{Date}

\begin{document}
\maketitle
\section{Notation}
\begin{itemize}
    \item $ L $ number of layer 
\end{itemize}
\section{Neural ODE}
Comme je connais vraiment rien en équa dif je vais sum up une vidéo. \begin{itemize}
    \item Neural ODE : mieux que les RNN pour prédire les time series
    \item Considère une NN non plus de manière discrète avec des layers/block de neurone mais d'une manière continue 
    \item Proche des Equa dif, et on peut utiliser la théorie de ça pour train à la place de la descente de gradiant 
    \item Pas appris beaucoup plus 
\end{itemize}

\section{Introduction}
\begin{itemize}
    \item Problème pour train les resnet en fonction de la profondeur: vanishing / exploding gradients.
    \item Solution classique : batch norm $\rightarrow$ fix la variance du signal $\rightarrow$ mais apporte d'autre problème
    \item Solution 2 : scaling factor dépendant de $ L $ mais comment ?? == objectif du papier
    \item 3 chapitres \begin{itemize}
        \item Scaling at initialization : \begin{itemize}
            \item Initialisation importante : avoid grad problems, larger learning rate (better generalization)
            \item Exploding gradients \textbf{during backprop}== $ \left\| \frac{\partial \mathcal{L}}{\partial h_0 }  \right\| \geq \geq \left\| \frac{\partial \mathcal{L}}{\partial h_L }  \right\|  $ with an high probability 
            \item $\rightarrow$ they study distibution and the choice of $ \alpha _L = \frac{1}{\sqrt[]{L}} $ 
        \end{itemize}
        \item The continuous approach : \begin{itemize}
            \item Si on pose $ \alpha _L = 1/L$  $\rightarrow$ ResNet= ODE
            \item Mais contradictoire avec le résultat de la section précédente $ \alpha _L = \frac{1}{\sqrt[]{L}} $ 
            \item En faite  $ \alpha _L = \frac{1}{\sqrt[]{L}} $ correspond au bon choix pour neural stochastic pdifferential equation (SDE). Qui correspond à un ResNet avec une initialisation particulière
        \end{itemize}
        \item Section 4 : test de differente valeur de $ \alpha _L $ dans le cadre SDE
    \end{itemize}
    \item Related work : \begin{itemize}
        \item Plein de papier sur $ \alpha _L $, plein de solution possible
        \item Nous on analise $ \alpha _L$ au moment de l'initialisation des paramètres 
        \item D'autre gens on trouvé $ \alpha _L = \frac{1}{\sqrt[]{L}} $ mais sans donner trop de math et sans fouiller les autres cas $ \alpha _L \ll \frac{1}{\sqrt[]{L}} $, $ \alpha _L \approx \frac{1}{\sqrt[]{L}} $, $ \alpha _L \gg \frac{1}{\sqrt[]{L}} $ et sans faire le lien avec les équa dif
        \item Des gens on déjà fait le lien avec les equa diff mais dans des cas moins général je crois
    \end{itemize}
\end{itemize}

\section{Scaling at initialization}
\subsection{Model and assumptions}
\subsubsection{Probabilistic setting at initialization }
\begin{itemize}
    \item Les paramètre du modèle est une collection iid de variable aléatoire $\rightarrow$ donc les états cachés $ h_0, \dots, h_L $ aussi (mais il sont martingale eux mais osef pour l'instant)
    \item La distribution initial des paramèter n'est pas dépendante de $ L $, donc indépendante de l'architecture du modèle considéré. Pratique !
    \item 
\end{itemize}

\subsubsection{Assumptions}
\begin{itemize}
    \item $ s^2 $ sub-Gaussian : $ \forall \lambda \in \mathbb{R}, \mathbb{E}(\lambda X) \leq  \exp (\frac{\lambda ^2 s^2 }{2})$  a sub-Gaussian distribution is a probability distribution with strong tail decay.
\end{itemize}

\begin{prop}[]
    les resnet du tableau vérifie $ A_1 $ et $ A_2 $ 
\end{prop}

\subsection{Probabilistic bounds on the norm of the hidden states}
Part3 du corrolaire  : Coeur du sujet, on vas regarder le comportement de $ \left\| h_L - h_0 \right\| / \left\| h_0 \right\|  $ en fonction de $ L \alpha _L ^2 $. Seul $ \beta = 1/2 $ donne une distribution non dégénéré à l'initialisation
\begin{itemize}
    \item $ L \alpha _L ^2 \ll 1 $  identity function
    \item $ L \alpha _L ^2 \gg 1 $  explosion du gradient avec forte proba
    \item $ L \alpha _L ^2 \approx 1 $, $ h_L $ fluctue autour de $ h_0 $ avec borne
\end{itemize}
Illustration en figure 1.

Figure 2 








\end{document}