@comment{x-kbibtex-encoding=utf-8}

@inproceedings{ACM-10.1145/3583133.3590735,
	address = {New York, NY, USA},
	author = {Wang, Shang and Tang, Huanrong and Ouyang, Jianquan},
	booktitle = {{Proceedings of the Companion Conference on Genetic and Evolutionary Computation}},
	isbn = {9798400701207},
	keywords = {genetic algorithm; transformer; multi-objective},
	location = {Lisbon, Portugal},
	month = jul,
	numpages = {4},
	pages = {691–694},
	publisher = {Association for Computing Machinery},
	series = {{GECCO '23 Companion}},
	title = {{A Transformer-based Neural Architecture Search Method}},
	url = {https://dl.acm.org/doi/pdf/10.1145/3583133.3590735},
	x-fetchedfrom = {ACM Digital Library},
	year = {2023}
}

@article{ieee9913476,
	abstract = {Transformer-based Deep Neural Network architectures have gained tremendous interest due to their effectiveness in various applications across Natural Language Processing (NLP) and Computer Vision (CV) domains. These models are the de facto choice in several language tasks, such as Sentiment Analysis and Text Summarization, replacing Long Short Term Memory (LSTM) model. Vision Transformers (ViTs) have shown better model performance than traditional Convolutional Neural Networks (CNNs) in vision applications while requiring significantly fewer parameters and training time. The design pipeline of a neural architecture for a given task and dataset is extremely challenging as it requires expertise in several interdisciplinary areas such as signal processing, image processing, optimization and allied fields. Neural Architecture Search (NAS) is a promising technique to automate the architectural design process of a Neural Network in a data-driven way using Machine Learning (ML) methods. The search method explores several architectures without requiring significant human effort, and the searched models outperform the manually built networks. In this paper, we review Neural Architecture Search techniques, targeting the Transformer model and its family of architectures such as Bidirectional Encoder Representations from Transformers (BERT) and Vision Transformers. We provide an in-depth literature review of approximately 50 state-of-the-art Neural Architecture Search methods and explore future directions in this fast-evolving class of problems.},
	author = {Chitty-Venkata, Krishna Teja and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K.},
	doi = {10.1109/ACCESS.2022.3212767},
	issn = {2169-3536},
	journal = {{IEEE Access}},
	keywords = {Transformers; Computer architecture; Convolutional neural networks; Computational modeling; Bit error rate; Search problems},
	pages = {108374–108412},
	publisher = {IEEE},
	title = {{Neural Architecture Search for Transformers: A Survey}},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=9913476; https://ieeexplore.ieee.org/document/9913476/},
	volume = {10},
	x-fetchedfrom = {IEEEXplore},
	year = {2022}
}

@misc{2301.08727v2,
	abstract = {  In the past decade, advances in deep learning have resulted in breakthroughs
in a variety of areas, including computer vision, natural language
understanding, speech recognition, and reinforcement learning. Specialized,
high-performing neural architectures are crucial to the success of deep
learning in these areas. Neural architecture search (NAS), the process of
automating the design of neural architectures for a given task, is an
inevitable next step in automating machine learning and has already outpaced
the best human-designed architectures on many tasks. In the past few years,
research in NAS has been progressing rapidly, with over 1000 papers released
since 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized
and comprehensive guide to neural architecture search. We give a taxonomy of
search spaces, algorithms, and speedup techniques, and we discuss resources
such as benchmarks, best practices, other surveys, and open-source libraries.
},
	archiveprefix = {arXiv},
	author = {White, Colin and Safari, Mahmoud and Sukthanker, Rhea and Ru, Binxin and Elsken, Thomas and Zela, Arber and Dey, Debadeepta and Hutter, Frank},
	comment = {published = 2023-01-20T18:47:24Z, updated = 2023-01-25T08:01:55Z},
	eprint = {2301.08727v2},
	month = jan,
	primaryclass = {cs.LG},
	title = {{Neural Architecture Search: Insights from 1000 Papers}},
	url = {http://arxiv.org/abs/2301.08727v2; http://arxiv.org/pdf/2301.08727v2},
	x-fetchedfrom = {arXiv.org},
	year = {2023}
}

@article{DBLP:journals/corr/abs-2102-07108,
	author = {Yan, Shen and Song, Kaiqiang and Feng, Zhe and Zhang, Mi},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	eprint = {2102.07108},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 02 Jun 2021 17:08:18 +0200},
	title = {{CATE:} Computation-aware Neural Architecture Encoding with Transformers},
	url = {https://arxiv.org/pdf/2102.07108.pdf; https://arxiv.org/abs/2102.07108; https://dblp.org/rec/journals/corr/abs-2102-07108.bib},
	volume = {abs/2102.07108},
	year = {2021}
}

@article{DBLP:journals/corr/abs-1810-00825,
	author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	eprint = {1810.00825},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 25 Mar 2022 07:32:05 +0100},
	title = {Set Transformer},
	url = {https://arxiv.org/pdf/1810.00825.pdf; http://arxiv.org/abs/1810.00825; https://dblp.org/rec/journals/corr/abs-1810-00825.bib},
	volume = {abs/1810.00825},
	year = {2018}
}

@ARTICLE{2023arXiv230516943A,
	author = {{An}, Sohyun and {Lee}, Hayeon and {Jo}, Jaehyeong and {Lee}, Seanie and {Hwang}, Sung Ju},
	title = "{DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning},
	year = 2023,
	month = may,
	eid = {arXiv:2305.16943},
	pages = {arXiv:2305.16943},
	doi = {10.48550/arXiv.2305.16943},
	archivePrefix = {arXiv},
	eprint = {2305.16943},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230516943A},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{DBLP:journals/corr/abs-1812-00332,
	author       = {Han Cai and
                  Ligeng Zhu and
                  Song Han},
	title        = {ProxylessNAS: Direct Neural Architecture Search on Target Task and
                  Hardware},
	journal      = {CoRR},
	volume       = {abs/1812.00332},
	year         = {2018},
	url          = {http://arxiv.org/abs/1812.00332},
	eprinttype    = {arXiv},
	eprint       = {1812.00332},
	timestamp    = {Tue, 24 Nov 2020 14:44:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1812-00332.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1807-11626,
	author       = {Mingxing Tan and
                  Bo Chen and
                  Ruoming Pang and
                  Vijay Vasudevan and
                  Quoc V. Le},
	title        = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},
	journal      = {CoRR},
	volume       = {abs/1807.11626},
	year         = {2018},
	url          = {http://arxiv.org/abs/1807.11626},
	eprinttype    = {arXiv},
	eprint       = {1807.11626},
	timestamp    = {Mon, 31 May 2021 15:34:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1807-11626.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}