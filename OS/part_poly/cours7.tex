\begin{proof}
    We have to use \textbf{averaged} uniform stab (this is a randomized algo!)

    Notation shortcuts : \begin{itemize}
        \item A($\mathcal{D}_n$, t steps) $\equiv \theta_t$
        \item A($\mathcal{D}_n^{(k)}$, t steps) $\equiv \tilde{\theta_t}$
    \end{itemize}

    $\forall t \geq 1$,
    %\begin{align*}
    %    \mathbb{E} [ \left\| & \theta_{t+1} - \tilde{\theta_{t+1}} \right\| | \mathcal{F}_t ] \\
    %    & = \mathbb{E}[ \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| \mathbb{1}_{i(t+1)= k} + \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| \mathbb{1}_{i(t+1)\neq  k}| \mathcal{F}_t] \\
    %    & = \frac{1}{n} \mathbb{E}[ \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| | \mathcal{F}_t, i(t+1)= k] + \frac{n-1}{n} \mathbb{E}[ \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| | \mathcal{F}_t, i(t+1) \neq k]
    %\end{align*}

    \begin{itemize}
        \item Conditionally to $i(t+1) = k$ ($\mathbb{E}[ \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| | \mathcal{F}_t, i(t+1)= k]$) , 
        \[
            \left\| \theta_{t+1} - \tilde{\theta_{t+1}} \right\| \leq \left\| \theta_{t} - \tilde{\theta_{t}} \right\| + \gamma _t ( \left\| \nabla \psi_k (\theta _k) \right\| + \left\| \nabla \tilde{\psi_k} (\tilde{\theta _k}) \right\|)
        .\]
        $\left\| \nabla \psi_k (\theta _k) \right\| \leq BR$ \\
        $\left\| \nabla \tilde{\psi_k} (\tilde{\theta _k}) \right\| \leq BR$

        \item Conditionally to $i(t+1) \neq  k$, the non-expensivness property gives that $\left\|  \theta_{t+1} - \tilde{\theta_{t+1}} \right\| \leq \left\|  \theta_{t} - \tilde{\theta_{t}} \right\|$ \\
        Therefore,
        
        \begin{align*}
            \mathbb{E}[ \left\| \tilde{\theta_{t+1}} - \theta_{t+1} \right\| | \mathcal{F}_t] &\leq \frac{1}{n} \left\| \theta_{t} - \tilde{\theta_{t}} \right\| + \frac{n-1}{n} \left\| \theta_{t} - \tilde{\theta_{t}} \right\| \\
            & = \left\| \theta_{t} - \tilde{\theta_{t}} \right\| + \frac{\gamma _ {t+1} 2BR}{n} \\
            & \dots \\
            & \leq \left\| \theta_0 - \tilde{\theta _0} \right\|  + \frac{\sum_{u=1}^{t+1}}{n}
        \end{align*}
            
        
        As previously, we have obtained a bound on $\left\| A(\mathcal{D}_n, T steps) - A(\mathcal{D_n^{(k)}}, T steps) \right\| $ witch leads to uniform stability with $\beta (n) = \frac{2B^2R^2 \sum_{t=1}^{T}\gamma _t}{n}$

        \textbf{Conclusion} What can be said about the generalization of SGD after T steps?
        \begin{itemize}
            \item Recall that for $T \leq n$, without replacement (one pass over the data), we actually minimize the generalization risk directly : we already obtained a rate in the previous lecture. 
            \item  (Charles) for T ...
        \end{itemize}
        
        
    \end{itemize}
\end{proof}