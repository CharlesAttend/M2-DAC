# Title page
* Explainable AI is Dead, Long Live Explainable AI!
    * Titre aguicheur, aime pas, deuxi√®me mieux
    * Hypothesis-driven Decision Support using Evaluative AI
* Papier pour aide √† la d√©cision
* Tim miller, chercheur australien, il publie sur des sujets assez vari√© en XAI
    * XAI 4 Multi agent, XAI 4 RL 
    * Humain-AI interaction
    * AI assisted d√©cision support

# Introduction
## Quick summary
* Changement paradigme XAI aide d√©cision
framework Evaluative AI :
* Centr√©e sur l'humain
* Au dela des recommandation
* En evaluant les hypoth√®se du d√©cideur
* **Mitiger l'Exc√®s de confiance dans ces systems de recommandation**

## Over/Under reliance
### D√©finition
* Aide √† la d√©cision, retrouve deux ph√©nom√®nes 
* Expliquer les d√©fininions : 
    * Exc√®s de confiance : accepter les recommandations, m√™me si faux
    * Manque de confiance : inverse : rejeter les recommendation, m√™me si vrais
* == Automation bias : 
    <!-- * Biais assez important, d√®s que l'humain a un r√¥le d'observateur sur les d√©cisions des machine et qu'il reste le d√©cideur -->
    * Qui a g√©n√©ralement des gros impacts
* Probl√®mes : 
    * **Pourquoi j'ai choisi ce papier **
    * La confiance excessive ‚Üí Probl√©matique
        * Au quotidient 
            * correcteur d'orthographe
            * Recrutement ; Pr√™t banquaire ect
            * Enjeux plus s√©rieux : Unit√© de soin intensif, aviation, centrale nucl√©aire
            * Plus g√©n√©ralement : craches boursier instantan√©
            * Algo mesure du risque de violence conjugale ‚Üí Exc√®s de confiance  ‚Üí erreur d'estimation du danger

    * Echo domain sciences cognitives licence, manque un peu
3:05 ; . ; 

### Causes
* Manque d'engagement cognitif, l'esprit humain cherche √† minimiser l'effort
* Algorithmic aversion : OSEF 
* + XAI qui explique
    * Une sorte de biais de confirmation sur les explications qu'on vas accepter ou rejeter

### Solutions
* Solution 1 : forcer l'engagement cognitif
    * par exemple, forcer les gens √† donenr leur d√©cision avant celle de la machine
    * pas giga efficace
    * pas trop appr√©ci√©
* Solution 2 : Un paradigme shift en XAI ü§îüòèüí°
    * C'est ce que propose ce papier
    * => Avant de parler de cela on vas d√©finir des crit√®res plus claire

## How we make decisions?
* Ce qu'on fait naturellement c'est identifier, comparer, choisir
* Des gens ont plus r√©flechis √† la chose 
    * **dans notre cadre**, pour les system d'aide √† la d√©cision, on peut r√©sum√© tout √ßa par ... DIAPO

## 10 cardinal decision issue
Bon system d'aide √† la d√©cision, besoin de 
* Option:  identifier, lister, r√©aliste/fesable
* Opinion & Possibilit√© : Proba et impact positif / negatif possible pour chaque options
* Compromis: comparer ce qu'on tout ce qu'on a dit au dessus
* Understand: comprendre le systeme d'aide √† la d√©cision

# Does current decision support align with those criteria?
* On vas comparer plusieurs system d'aide √† la d√©cision et voir si ils respectent ces crit√®res
5:30 ; . ; 
## No explanotory information
* Cas classique d'automatisation des d√©cisions : *d√©crire un peu*
* Comme dit pr√©c√©dement, les gens tendent √† soit ignorer le system soit accepter des mauvaise d√©cisions
* Le d√©cideur : Calibration de la confiance uniquement sur :
    * l'accuracy du model 
    * Son exp√©rtise
* => Novice : se repose sur le systems // expert : utilise leur propre expertise 
* Mais ne coche aucune des cases
    * N'aide pas √† identifier les autres options probable 
    * Opinion uniquement autour de la recommandation
    * N'aide pas √† faire des compromis
    * Pas de expliquabilit√©
* Mais est-ce que c'est quand m√™me utile ?? OUI 
    * Si on est d'accord avec le system tout roule 
    * Si on est pas d'accord, on peut reconsid√©rer le choix
    * Et toujours faire de meilleurs d√©cision

## With explanatory information
* Cette fois ci avec un outil de XAI 
* Coche plus de case 
    * Avec les outils XAI, on peut comprendre le mod√®le 
    * et faire des compromis par exemple avec SHAP ou les counterfactual
    * N'aide pas √† identifier les autres options probables
    * Jugement et possibilit√© uniquement autour de la recommandation
* => Mais c'est toujours pour d√©fendre la recommandation
* Est-ce que c'est quand m√™me utile ?? OUI
    * Pour les m√™me raison que pr√©c√©dement :
        * Si on est pas d'accords on peut regarder et √ßa donne potentiellement de meilleurs d√©cision
    * Mais en pratique c'est pas souvent le cas
* Un model interpr√©table coche uniquement la derni√®re case 

## Cognitive for√ßing
* Cognitive forcing : d√©cideur donne d√©cision avant machine
* C'est le cas qui coche le plus de case dans le paradigme actuel mais y'a toujours des probl√®mes
    * d√©cideur voie plus d'option : forc√© de les chercher 
    * Toujours Avec les outils XAI, on peut comprendre le mod√®le 
    * et faire des compromis par exemple avec SHAP ou les counterfactual
* Mais le system est toujours centr√© sur sa recommandation 
* d√®s que centr√© autour de la recommandation, on ne fait que partiellement cocher les cases
* => Sortir de ce paradigme de recommandation unique => evaluative AI

II: 8:27
# The evaluative AI framework
* D√©crire : boucle, d√©cideur -> HP -> feedback
* Le paradigme est invers√© : 
    * c'est la machine qui donne son avis sur la d√©cision du *decision-maker* 
    * Et non le d√©cision maker qui donne son avis sur la d√©cision de la machine 
II: 9:09

## Properties
* Exemple d'interface 
    * Potentiel m√©lanome ? 
    * Vu sur toutes les options possible 
    * int√©raction avec l'utilisateur 
    * hypoth√®se pour, hypoth√®se contre
  
## Zoom on properties
* Naturellement leur mod√®le coche toutes les cases 
* Option
    <!-- * Donne plusieurs option, sans forc√©ment dire la plus probable / leur probabilit√© 
    * Overview des possibilit√©, r√©duit un peu l'information -->
<!-- * Jugement et posibilit√© 
    * Ici c'est bien on support l'opinion du d√©cideur, 
    * Le system ne donne pas son opinion 
    * Feedback -->
* Trade-off
    * Je trouve que c'est ici que √ßa r√©ussi le mieux 
    * Pour ou contre assez clair pour permettre au d√©cideur un bonne overview
    * Et en faite les "bon d√©cideur" sont les personnes qui regarde toujours les arguments qui vont contre leurs conclusions initiales 
* => extrapolation sur de l'IRL 
    * Quand d√©cision complexe, type choix de stage, orientation
        * Si on reste regarde tous les pours et contre, j'trouve √ßa devient vite bourbier 
        * Alors que maybe se fier a l'instincs et indentifier les contre serait plus efficace
    * les discutions IRL ? 
        * la cl√© √ßa reste d'√™tre √† l'√©coute et tourner autour de l'opinion de l'autre sans forc√©ment directement relate sur des pov personel

## Limits
* Pourquoi les gens s'engagerai avec ce system et pas les autres m√©thode 
    * Plus de controle 
    * Proche de la mani√®re dont on fait des d√©cisions (identifier, comparer, choisir)
    * X : pas de preuve de √ßa dans le papier (en psychologie √ßa serait pas pass√©, jsp pour Humain-AI interaction)
* M√©thode qui charge mentalement le d√©cideur 
    * X: toujours la moins aim√© surement 
    * Mais auteur se d√©fend en disant qu'il y a quand m√™me moins d'info √† consid√©rer

## Mes critiques  
* Les crit√®res sont dur √† diff√©rencier 
    * Y'en a 10 de base, il en garde 6, 1 n'est jamais remplis, et 2 fusionne en 1 car proche (opinion et possibilit√©)
    * Des fois c'est dur de s'y retrouver, le tableau r√©sum√© est pas forc√©ment accords avec que qui est dit dans le texte, 
* Quand j'ai √©t√© voir la page wikipedia de l'automation bias, elle est assez remplis et l'autheur en parle pas du tout. Y'as pas mal d'autre facteur d√©crit et j'arrive pas √† voir pourquoi y'a pas un mot dessus dans le papier 
    * A la place l'intro parle du r√©sonnement abductif pour appuyer son mod√®le comme un mod√®le proche de la mani√®re naturel de la d√©cision
    * Alors qu'il aurait eu la place car beaucoup de r√©p√©tition dans son papier

## Mes points forts du papier
* S'attaque √† un vrais probl√®me 
* Avec une proposition forte, position pas facile √† tenir
* Pas d'exp√©rience pour appuyer l'√©valuative AI 
    * mais donne une liste exhaustive de piste de recherche dans la direction de l'√©valuative AI

# CONCLUSION
* auteur propose de changer de voix pour le XAI appliqu√© l'aide √† la d√©cision 
* Qu'il faut arreter d'expliquer les recommendation et se focus sur l'utilisateur et ces hypoth√®ses
* En se rapprochant de la mani√®re dont on prends naturellement des d√©cisions 
II: 14:42 ; 12:47



<!-- --- Je garde pour les questions au cas o√π -->
## Differences with cognitive forcing 
* Apparament √ßa ressemblerai pas mal au technique de cognitive for√ßing 
* Les auteurs essaye plusieurs fois de se diff√©rencier √† travers le papier
* Ici la cl√© c'est que le d√©cideur est en position de contr√¥le face √† la machine, "machine in the loop"
* √©galement que √ßa suit un chemin de d√©cision plus naturelle

## Long live XAI 
* Le titre est pas vraiment clair au premi√®re abord mais il se d√©fend
* L'auteur ne veut pas se s√©parer de l'XAI ou faire une refonte
* Il veut am√©liorer une petite branche de l'XAI 
    * Evaluative AI $ \subset $ XAI 
* XAI + approche bas√© sur la recommendation sont bien et adapt√© dans certains cas 
    * Making decision at scale 
* Il faudra toujours un model recommendation based pour n'importe quelle XAI technique
* Outil de XAI existant -> d√©j√† adapt√© √† l'evaluative AI
    * Counterfactuals 
    * Feature importance (SHAP)
    * Wieghts of Evidence, case-based reasoning techniques 
II: 12:06
<!-- --- -->