{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction de règles contre-factuelles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "sU5XKbvUXSHcq2k7cg24mT",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adresse</th>\n",
       "      <th>Majeur?</th>\n",
       "      <th>Nationalite</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paris</td>\n",
       "      <td>oui</td>\n",
       "      <td>Francais</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paris</td>\n",
       "      <td>non</td>\n",
       "      <td>Francais</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montpellier</td>\n",
       "      <td>oui</td>\n",
       "      <td>Italien</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paris</td>\n",
       "      <td>oui</td>\n",
       "      <td>Suisse</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>non</td>\n",
       "      <td>Italien</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>non</td>\n",
       "      <td>Francais</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>oui</td>\n",
       "      <td>Francais</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Montpellier</td>\n",
       "      <td>oui</td>\n",
       "      <td>Suisse</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adresse Majeur? Nationalite  Label\n",
       "0        Paris     oui    Francais      1\n",
       "1        Paris     non    Francais     -1\n",
       "2  Montpellier     oui     Italien      1\n",
       "3        Paris     oui      Suisse     -1\n",
       "4   Strasbourg     non     Italien     -1\n",
       "5   Strasbourg     non    Francais     -1\n",
       "6   Strasbourg     oui    Francais      1\n",
       "7  Montpellier     oui      Suisse     -1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \".\"\n",
    "elections_data_path = f\"{PATH}/data/elections.csv\"\n",
    "\n",
    "# Séparer les caractéristiques et les étiquettes\n",
    "elections_data = pd.read_csv(elections_data_path)\n",
    "X = elections_data.drop(\"Label\", axis=1)\n",
    "y = elections_data[\"Label\"]\n",
    "\n",
    "elections_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nbgdVeUsDTR3u0AnCbAtJW",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Criterion:\n",
    "    def __init__(self, criterion) -> None:\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        if self.criterion == \"entropy\":\n",
    "            return self.shannon(*args, **kwds)\n",
    "        elif self.criterion == \"gini\":\n",
    "            return self.gini(*args, **kwds)\n",
    "        elif self.criterion == \"ambiguity\":\n",
    "            return self.ambiguity(*args, **kwds)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown impurity criterion: {self.criterion}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def shannon(labels):\n",
    "        \"\"\"Permet de calculer l'entropie de Shannon.\n",
    "\n",
    "        Renvoie : $\\sum_{k=1}^{K} p(c_k|v_j)\\log p(c_k|v_j)$.\n",
    "\n",
    "        \\[\n",
    "            H_S(Y|X) = -\\sum{j=1}^{m} p(v_j) \\cdot \\sum_{k=1}^{K} p(c_k|v_j)\\log p(c_k|v_j)\n",
    "        \\]\n",
    "        \"\"\"\n",
    "        label_counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in label_counts.values()]\n",
    "        return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def gini(labels):\n",
    "        \"\"\"Permet de calculer l'indice de diversité de Gini.\n",
    "\n",
    "        Renvoie : $(1 - \\sum_{k=1}^{K} p(c_k | v_j)^2)$.\n",
    "\n",
    "        \\[\n",
    "            H_G(Y | X) = \\sum_{j=1}^{m} p(v_j) \\cdot (1 - \\sum_{k=1}^{K} p(c_k | v_j)^2)\n",
    "        \\]\n",
    "        \"\"\"\n",
    "        label_counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in label_counts.values()]\n",
    "        return 1 - sum(p**2 for p in probabilities)\n",
    "\n",
    "    @staticmethod\n",
    "    def ambiguity(labels):\n",
    "        \"\"\"Permet de calculer la mesure d'ambiguïté [Yuan & Shaw, 1995].\n",
    "\n",
    "        Renvoie la mesure de non-spécificité.\n",
    "\n",
    "        \\[\n",
    "            H_Y(Y|X) = \\sum_{j=1}^{m} p(v_j) \\cdot g(\\Pi(Y|v_j))\n",
    "        \\]\n",
    "\n",
    "        avec $g$ une mesure de non-spécificité :\n",
    "\n",
    "        \\[\n",
    "            g(\\Pi(C|v_j)) = \\sum_{i=2}^{K} \\pi_i \\cdot (\\log(i) - \\log(i-1))\n",
    "        \\]\n",
    "\n",
    "        où $\\pi$ est obtenue de la façon suivante :\n",
    "        1. on ordonne les $p(c_k|v_j) dans l'odre décroissant ;\n",
    "        2. on définit $\\pi_i = \\frac{p_i}{p_1}$ pour tout $i = 1, ..., K$.\n",
    "        \"\"\"\n",
    "        label_counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in label_counts.values()]\n",
    "\n",
    "        # Calculate π\n",
    "        probabilities.sort(reverse=True)\n",
    "        pi = [prob / probabilities[0] for prob in probabilities]\n",
    "\n",
    "        # Calculate the measure of non-specificity g(Π(C|v_j))\n",
    "        g = sum(pi[i] * (np.log2(i + 1) - np.log2(i)) for i in range(1, len(pi) + 1))\n",
    "\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "class SymbolicDecisionTree:\n",
    "    \"\"\"Un arbre de décision prenant en compte des données symboliques.\"\"\"\n",
    "\n",
    "    def __init__(self, criterion=\"gini\"):\n",
    "        self.tree = None\n",
    "        self.criterion = Criterion(criterion)\n",
    "\n",
    "    class Node:\n",
    "        \"\"\"Un noeud de l'arbre de décision symbolique.\"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            feature=None,\n",
    "            value=None,\n",
    "            true_branch=None,\n",
    "            false_branch=None,\n",
    "            result=None,\n",
    "            entropy=None,\n",
    "        ):\n",
    "            self.feature = feature\n",
    "            self.value = value\n",
    "            self.true_branch = true_branch\n",
    "            self.false_branch = false_branch\n",
    "            self.result = result\n",
    "            self.entropy = entropy\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Construit l'arbre de décision à partir de l'ensemble d'apprentissage (X, y).\"\"\"\n",
    "        training_data = pd.concat([X, y], axis=1)  # Combine features and labels\n",
    "        self.tree = self.build_tree(training_data)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Prédit les classes pour des échantillons d'entrée.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            predictions = [\n",
    "                self.predict_single_entry(entry) for _, entry in X.iterrows()\n",
    "            ]\n",
    "        elif isinstance(X, pd.Series):\n",
    "            predictions = self.predict_single_entry(X)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"type {type(X)} not supported in predict, please implement or convert to a pandas object\"\n",
    "            )\n",
    "        return predictions\n",
    "\n",
    "    def predict_single_entry(self, entry):\n",
    "        \"\"\"Prédit la classe pour une seule entrée.\"\"\"\n",
    "        node = self.tree\n",
    "        while node.result is None:\n",
    "            if entry[node.feature] == node.value:\n",
    "                node = node.true_branch\n",
    "            else:\n",
    "                node = node.false_branch\n",
    "        return node.result\n",
    "\n",
    "    def predict_xai(self, entry):\n",
    "        \"\"\"Fournit une explication pour un exemple donné.\"\"\"\n",
    "        explanation, _ = self.trace_tree(entry, self.tree, [])\n",
    "        return explanation\n",
    "\n",
    "    def find_counterfactuals(self, class_to_exclude):\n",
    "        \"\"\"Trouve les règles contre-factuelles de l'arbre pour un exemple donné.\n",
    "\n",
    "        :param class_to_exclude: La classe de l'exemple à interpréter.\n",
    "        \"\"\"\n",
    "        paths = self.untrace_tree(self.tree, [])\n",
    "        filtered_paths = [path for path in paths if path[-1] != class_to_exclude]\n",
    "        return filtered_paths\n",
    "\n",
    "    def explain_counterfactuals(self, entry, class_to_exclude):\n",
    "        \"\"\"Renvoie les règles contre-factuelles, la classe prédite et le nombre de tests\n",
    "        invalidés pour un exemple donné.\n",
    "\n",
    "        :param entry: L'exemple à interpréter.\n",
    "        :param class_to_exclude: La classe de l'exemple à interpréter.\n",
    "        \"\"\"\n",
    "        formatted_path = []\n",
    "        counterfactuals = self.find_counterfactuals(class_to_exclude)\n",
    "        unsatisfied_rules = self.count_rules(entry, counterfactuals)\n",
    "\n",
    "        # Trouve les règles qui minimisent le nombre de tests invalidés par l'exemple\n",
    "        min_count = min(unsatisfied_rules)\n",
    "        best_indices = [\n",
    "            i for i, count in enumerate(unsatisfied_rules) if count == min_count\n",
    "        ]\n",
    "        best_rules = [counterfactuals[i] for i in best_indices]\n",
    "\n",
    "        for path, rule_count in zip(counterfactuals, unsatisfied_rules):\n",
    "            # formatted_path.append(self.format_path(path[0], path[1]))\n",
    "            formatted_path.append(\n",
    "                (\n",
    "                    [\n",
    "                        f\"{feature} {'!=' if not decision else '=='} {value}\"\n",
    "                        for feature, decision, value in path[0]\n",
    "                    ],\n",
    "                    path[1],\n",
    "                    rule_count,\n",
    "                )\n",
    "            )\n",
    "        return formatted_path\n",
    "\n",
    "    def explain_notebook(self, entry, class_to_exclude):\n",
    "        desc = \", \".join(\n",
    "            [\n",
    "                f\"{feature} = {value}\"\n",
    "                for feature, value in zip(entry.index, entry.values)\n",
    "            ]\n",
    "        )\n",
    "        explanation = f\"<p><strong>Description</strong> : {desc}, Classe = {class_to_exclude}</p>\"\n",
    "        # Régle de classification\n",
    "        prediction = self.predict(entry)\n",
    "        explanation += f\"<p><strong>Règle de classification</strong> : l'exemple est classé {prediction}<br/>\"\n",
    "        classification_rule = self.predict_xai(entry)\n",
    "        classification_rule = [path.split(\" \") for path in classification_rule]\n",
    "        explanation += self.format_path(classification_rule, class_to_exclude, entry)\n",
    "        explanation += f\"</p><p><strong>Règles contre-exemples</strong> : celles dont la conclusion n'est pas {prediction}<br/>\"\n",
    "\n",
    "        # Règles contre-exemples\n",
    "        counterfactuals = self.explain_counterfactuals(entry, class_to_exclude)\n",
    "        for cf in counterfactuals:\n",
    "            rules = [rule.split(\" \") for rule in cf[0]]\n",
    "            explanation += self.format_path(rules, cf[1], entry)\n",
    "            explanation += \"<br/>\"\n",
    "        explanation += \"</p>\"\n",
    "\n",
    "        return display(HTML(explanation))\n",
    "\n",
    "    @staticmethod\n",
    "    def format_path(path, result, entry):\n",
    "        \"\"\"Formatte un chemin en un human-readable string représentant les règles de\n",
    "        décision. Les règles qui diffèrent des valeurs d'entrée sont mises en évidence\n",
    "        en couleur.\n",
    "        \"\"\"\n",
    "        # TODO: en fait c'est faux avec la couleur pcq on a des branches true et false\n",
    "        # qui prennent pas en compte toutes les valeurs des features\n",
    "        conditions = []\n",
    "        for feature, decision, value in path:\n",
    "            verbe = \"est\" if decision == \"==\" else \"n'est pas\"\n",
    "            if entry[feature] != value:\n",
    "                condition = (\n",
    "                    f\"<span style='color: #648FFF;'>{feature} {verbe} {value}</span>\"\n",
    "                )\n",
    "            else:\n",
    "                condition = f\"{feature} {verbe} {value}\"\n",
    "            conditions.append(condition)\n",
    "\n",
    "        if len(conditions) > 1:\n",
    "            conditions_str = \", \".join(conditions[:-1]) + \" et \" + conditions[-1]\n",
    "        else:\n",
    "            conditions_str = conditions[0]\n",
    "\n",
    "        return f\"Si {conditions_str} alors classe {result}.\"\n",
    "\n",
    "    def build_tree(self, data):\n",
    "        # If there's no data, or if all targets are the same,\n",
    "        # return a leaf node with the result\n",
    "        if len(data) == 0:\n",
    "            return self.Node()\n",
    "\n",
    "        current_uncertainty = self.criterion(data.iloc[:, -1])\n",
    "        best_gain = 0\n",
    "        best_criteria = None\n",
    "        best_sets = None\n",
    "\n",
    "        feature_count = len(data.columns) - 1  # number of attributes\n",
    "\n",
    "        for col in range(feature_count):  # for each feature\n",
    "            feature_values = data.iloc[:, col].unique()  # unique values\n",
    "            for val in feature_values:  # for each value\n",
    "                partitioned_data = self.partition(data, data.columns[col], val)\n",
    "\n",
    "                # Information gain\n",
    "                p = float(partitioned_data[0].shape[0]) / data.shape[0]\n",
    "                gain = (\n",
    "                    current_uncertainty\n",
    "                    - p * self.criterion(partitioned_data[0].iloc[:, -1])\n",
    "                    - (1 - p) * self.criterion(partitioned_data[1].iloc[:, -1])\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    gain > best_gain\n",
    "                    and len(partitioned_data[0]) > 0\n",
    "                    and len(partitioned_data[1]) > 0\n",
    "                ):\n",
    "                    best_gain = gain\n",
    "                    best_criteria = (data.columns[col], val)\n",
    "                    best_sets = partitioned_data\n",
    "\n",
    "        if best_gain > 0:\n",
    "            true_branch = self.build_tree(best_sets[0])\n",
    "            false_branch = self.build_tree(best_sets[1])\n",
    "            return self.Node(\n",
    "                feature=best_criteria[0],\n",
    "                value=best_criteria[1],\n",
    "                true_branch=true_branch,\n",
    "                false_branch=false_branch,\n",
    "                entropy=current_uncertainty,\n",
    "            )\n",
    "        else:\n",
    "            # We're at a leaf, determine the outcome most frequent class\n",
    "            outcome = data.iloc[:, -1].value_counts().idxmax()\n",
    "            return self.Node(result=outcome, entropy=current_uncertainty)\n",
    "\n",
    "    @staticmethod\n",
    "    def partition(data, feature, value):\n",
    "        true_data = data[data[feature] == value]\n",
    "        false_data = data[data[feature] != value]\n",
    "        return true_data, false_data\n",
    "\n",
    "    def trace_tree(self, entry, node, explanation=[]):\n",
    "        \"\"\"Collecte le chemin mené par l'arbre pour prédire un exemple donné.\n",
    "\n",
    "        :param entry: L'exemple à interpréter.\n",
    "        :param node: Le noeud actuel de l'arbre.\n",
    "        :param explanation: Le chemin mené par l'arbre.\n",
    "        \"\"\"\n",
    "        if node.result is not None:\n",
    "            return explanation, node.result\n",
    "\n",
    "        if entry[node.feature] == node.value:\n",
    "            explanation.append(f\"{node.feature} == {node.value}\")\n",
    "            return self.trace_tree(entry, node.true_branch, explanation)\n",
    "        else:\n",
    "            explanation.append(f\"{node.feature} != {node.value}\")\n",
    "            return self.trace_tree(entry, node.false_branch, explanation)\n",
    "\n",
    "    def untrace_tree(self, node, path=[]):\n",
    "        \"\"\"Traverse récursivement l'arbre de décision pour collecter les chemins menant\n",
    "        aux noeuds feuilles.\n",
    "\n",
    "        :param node: Le noeud actuel de l'arbre.\n",
    "        :param path: Le chemin emprunté pour atteindre le nœud courant.\n",
    "        :return: Une liste de chemins, où chaque chemin est une liste de tuples\n",
    "        contenant (feature, decision, value) menant à une feuille.\n",
    "        \"\"\"\n",
    "        if node.result is not None:\n",
    "            # Return the path with the result at the end\n",
    "            return [(path, node.result)]\n",
    "\n",
    "        paths = []\n",
    "        if node.false_branch is not None:\n",
    "            # Traverse the false branch\n",
    "            false_path = deepcopy(path)\n",
    "            false_path.append((node.feature, False, node.value))\n",
    "            paths.extend(self.untrace_tree(node.false_branch, false_path))\n",
    "\n",
    "        if node.true_branch is not None:\n",
    "            # Traverse the true branch\n",
    "            true_path = deepcopy(path)\n",
    "            true_path.append((node.feature, True, node.value))\n",
    "            paths.extend(self.untrace_tree(node.true_branch, true_path))\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def count_rules(self, entry, counterfactuals):\n",
    "        \"\"\"Compte le nombre de tests invalidés des règles contre-factuelles pour un\n",
    "        échantillon donné.\n",
    "\n",
    "        :param entry: L'exemple à interpréter.\n",
    "        :param counterfactuals: L'ensemble les règles contre-factuelles.\n",
    "        \"\"\"\n",
    "        unsatisfied_rules_count = []\n",
    "        for path in counterfactuals:\n",
    "            count = sum(\n",
    "                1\n",
    "                for feature, decision, value in path[0]\n",
    "                if (decision is False and entry[feature] == value)\n",
    "                or (decision is True and entry[feature] != value)\n",
    "            )\n",
    "            unsatisfied_rules_count.append(count)\n",
    "        return unsatisfied_rules_count\n",
    "\n",
    "    def display_tree(self, node=None, indent=\"\", branch=\"\"):\n",
    "        \"\"\"Visualise la structure arborescente de l'arbre dans un format clair et\n",
    "        organisé.\n",
    "\n",
    "        Format de sortie :\n",
    "        \"├──\" indique un noeud qui confirme la condition.\n",
    "        \"└──\" indique un noeud qui ne confirme pas la condition.\n",
    "        Les feuilles affichent le résultat de la classe.\n",
    "        Les noeuds internes affichent le critère de décision et son entropie.\n",
    "\n",
    "        Exemple :\n",
    "        feature_name1 == feature_value1? (Entropy = 0.1234)\n",
    "        ├── Class : Class_A\n",
    "        └── feature_name2 == feature_value2? (Entropy = 0.5678)\n",
    "            ├── Class : Class_B\n",
    "            └── Class : Class_C\n",
    "\n",
    "        Dans cet exemple :\n",
    "        - Si feature_name1 == feature_value1, l'arbre de décision le classe comme\n",
    "        'Class_A'.\n",
    "        - Sinon, il vérifie si feature_name2 == feature_value2 ; si c'est vrai, il est\n",
    "        classé comme 'Class_B', et si c'est faux, il est classé comme 'Class_C'.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        # Base case: if it's a leaf node, print the result and return\n",
    "        if node.result is not None:\n",
    "            print(f\"{indent}{branch}Class: {node.result}\")\n",
    "            return\n",
    "\n",
    "        # Print the criterion for the current node\n",
    "        print(\n",
    "            f\"{indent}{branch}{node.feature} == {node.value}? (Entropy = {node.entropy:.4f})\"\n",
    "        )\n",
    "\n",
    "        # Recursive case: print the true and false branches\n",
    "        new_indent = indent + (\"│   \" if branch == \"├── \" else \"    \")\n",
    "\n",
    "        # For true branch, use '├──'\n",
    "        self.display_tree(node.true_branch, new_indent, \"├── \")\n",
    "\n",
    "        # For false branch, use '└──'\n",
    "        self.display_tree(node.false_branch, new_indent, \"└── \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "E2DTytp1Ds7XaHOozQk3Tr",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialiser et entraîner l'arbre de décision symbolique\n",
    "tree_xai = SymbolicDecisionTree()\n",
    "tree_xai.fit(X_train, y_train)\n",
    "\n",
    "# Évaluer la précision de l'arbre de décision\n",
    "predictions = tree_xai.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalite == Suisse? (Entropy = 0.4800)\n",
      "    ├── Class: -1\n",
      "    └── Majeur? == oui? (Entropy = 0.4444)\n",
      "        ├── Class: 1\n",
      "        └── Class: -1\n"
     ]
    }
   ],
   "source": [
    "tree_xai.display_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nationalite != Suisse', 'Majeur? == oui']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, sample_label = X.iloc[0], y.iloc[0]\n",
    "\n",
    "tree_xai.predict_xai(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Nationalite != Suisse', 'Majeur? != oui'], -1, 1),\n",
       " (['Nationalite == Suisse'], -1, 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_xai.explain_counterfactuals(sample, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Description</strong> : Adresse = Paris, Majeur? = oui, Nationalite = Francais, Classe = 1</p><p><strong>Règle de classification</strong> : l'exemple est classé 1<br/>Si <span style='color: #648FFF;'>Nationalite n'est pas Suisse</span> et Majeur? est oui alors classe 1.</p><p><strong>Règles contre-exemples</strong> : celles dont la conclusion n'est pas 1<br/>Si <span style='color: #648FFF;'>Nationalite n'est pas Suisse</span> et Majeur? n'est pas oui alors classe -1.<br/>Si <span style='color: #648FFF;'>Nationalite est Suisse</span> alors classe -1.<br/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree_xai.explain_notebook(sample, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération de bases d’apprentissage et d’explications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import copy\n",
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "# # Nom des fonctions issus de la notation du cours\n",
    "# class CustomExplainer:\n",
    "#     def __init__(self, class_names, card_Z=10000):\n",
    "#         self.class_names = class_names\n",
    "#         self.card_Z = card_Z\n",
    "\n",
    "#     def generate_z(self, x_ref):\n",
    "#         Z = []  # list of all new exemple\n",
    "#         weights = []\n",
    "#         card_x_ref = len(x_ref)\n",
    "#         sigma = int(\n",
    "#             0.75 * np.sqrt(card_x_ref)\n",
    "#         )  # on change maximum sigma nombre de mots dans la phrase\n",
    "#         for _ in range(self.card_Z):\n",
    "#             z_i = copy(x_ref)\n",
    "#             nb_word_to_change = np.random.randint(0, sigma)\n",
    "#             index_to_remove = np.random.randint(0, card_x_ref, size=nb_word_to_change)\n",
    "#             for i in index_to_remove:\n",
    "#                 splited_text_tmp[i] = \"\"\n",
    "#             Z.append(\" \".join(splited_text_tmp))\n",
    "#             weights.append(1 - nb_word_to_change / card_x_ref)\n",
    "#         return Z, weights\n",
    "\n",
    "#     def explain_instance(self, text_instance, classifier_fn, num_features=None):\n",
    "#         modified_text_instances, weights = self.generate_z(text_instance)\n",
    "#         prob = classifier_fn(modified_text_instances).max(1)\n",
    "#         clf = Ridge(fit_intercept=True)\n",
    "#         clf.fit(\n",
    "#             self.vectorizer.transform(modified_text_instances),\n",
    "#             prob,\n",
    "#             sample_weight=weights,\n",
    "#         )\n",
    "#         return pd.Series(clf.coef_, index=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# explainer = CustomLimeTextExplainer(\n",
    "#     class_names=newsgroups_test.target_names, vectorizer=vectorizer\n",
    "# )\n",
    "# coef = explainer.explain_instance(\n",
    "#     newsgroups_test.data[idx], c.predict_proba, num_features=6\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode LORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from distances import simple_match_distance\n",
    "\n",
    "\n",
    "class CustomExplainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_set: pd.DataFrame,\n",
    "        model,\n",
    "        distance,\n",
    "        pc=0.5,\n",
    "        pm=0.5,\n",
    "        nb_genetics=100,\n",
    "        N=1000,\n",
    "    ):\n",
    "        self.N = N\n",
    "        self.pc = pc\n",
    "        self.pm = pm\n",
    "        self.nb_genetics = nb_genetics\n",
    "        self.model = model\n",
    "        self.distance = distance\n",
    "\n",
    "        self.value_distribution = {}\n",
    "        for col in test_set.columns:\n",
    "            vc = test_set[col].value_counts(normalize=True)\n",
    "            self.value_distribution[col] = (vc.index, vc.values)\n",
    "\n",
    "    def geneticNeigh(self, x: pd.Series, same: bool):\n",
    "        if same:\n",
    "            fitness = self.fitness_same\n",
    "        else:\n",
    "            fitness = self.fitness_dif\n",
    "\n",
    "        P = pd.DataFrame([x] * self.N)  # Population init\n",
    "        M = self.evaluate(x, P, fitness)\n",
    "        top_M = np.flip(M.argsort())\n",
    "        for _ in tqdm(range(self.nb_genetics)):\n",
    "            P = P.iloc[top_M].head(self.N)\n",
    "            P = self.crossover(P)\n",
    "            P = self.mutate(P)\n",
    "\n",
    "            M = self.evaluate(x, P, fitness)\n",
    "            top_M = np.flip(M.argsort())\n",
    "        return P\n",
    "\n",
    "    def evaluate(self, x, P: pd.DataFrame, fitness):\n",
    "        x_hat = self.model.predict(x)\n",
    "        M_list = []\n",
    "        for _, z_i in P.iterrows():\n",
    "            M_list.append(fitness(x, x_hat, z_i))\n",
    "        return np.array(M_list)\n",
    "\n",
    "    def fitness_same(self, x, x_hat, z_i):\n",
    "        z_i_hat = self.model.predict(z_i)\n",
    "        return (z_i_hat == x_hat) + (1 - self.distance(x, z_i)) - ((x == z_i).all())\n",
    "\n",
    "    def fitness_dif(self, x, x_hat, z_i):\n",
    "        z_i_hat = self.model.predict(z_i)\n",
    "        return (z_i_hat != x_hat) + (1 - self.distance(x, z_i)) - ((x == z_i).all())\n",
    "\n",
    "    def mutate(\n",
    "        self,\n",
    "        P: pd.DataFrame,\n",
    "    ):\n",
    "        idx_to_mutate = np.random.randint(0, len(P), size=int(len(P) * self.pm))\n",
    "        mutations = P.iloc[idx_to_mutate].apply(self.mutate_one, axis=1)\n",
    "        P_mutated = pd.concat((mutations, P))\n",
    "        return P_mutated\n",
    "\n",
    "    def mutate_one(self, parent):\n",
    "        \"\"\"\n",
    "        Ici je fais deux mutation, il est possible d'en faire qu'une\n",
    "        Le text du diapo en fait une, mais l'image en fait deux\n",
    "        \"\"\"\n",
    "        attribute_list = parent.index\n",
    "        att_1 = np.random.choice(attribute_list)\n",
    "        att_2 = np.random.choice(attribute_list.drop(att_1))\n",
    "        children = parent.copy()\n",
    "        children[att_1] = np.random.choice(\n",
    "            self.value_distribution[att_1][0], p=self.value_distribution[att_1][1]\n",
    "        )\n",
    "        children[att_2] = np.random.choice(\n",
    "            self.value_distribution[att_2][0], p=self.value_distribution[att_2][1]\n",
    "        )\n",
    "        return children\n",
    "\n",
    "    def crossover(self, P: pd.DataFrame):\n",
    "        nb_parents = int(len(P) * self.pm)\n",
    "        idx_parents = np.random.randint(0, len(P), size=(2, nb_parents // 2))\n",
    "        crossovers = pd.concat(\n",
    "            (\n",
    "                self.crossover_one(P.iloc[idx_parents[0, i]], P.iloc[idx_parents[1, i]])\n",
    "                for i in range(nb_parents // 2)\n",
    "            )\n",
    "        )\n",
    "        P_mutated = pd.concat((crossovers, P))\n",
    "        return P_mutated\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover_one(parent_1, parent_2):\n",
    "        attribute_list = parent_1.index\n",
    "        att_1 = np.random.choice(attribute_list)\n",
    "        att_2 = np.random.choice(\n",
    "            attribute_list.drop(att_1)\n",
    "        )  # on veut deux attribut différent\n",
    "        att_list = [att_1, att_2]\n",
    "\n",
    "        children_1 = parent_1.copy()\n",
    "        children_1[att_list] = parent_2[att_list]\n",
    "\n",
    "        children_2 = parent_2.copy()\n",
    "        children_2[att_list] = parent_1[att_list]\n",
    "\n",
    "        return pd.DataFrame((children_1, children_2))\n",
    "\n",
    "\n",
    "custom_explainer = CustomExplainer(X, tree_xai, simple_match_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "Z_true = custom_explainer.geneticNeigh(X.iloc[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "Z_false = custom_explainer.geneticNeigh(X.iloc[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = pd.concat((Z_true, Z_false)).reset_index(drop=True)\n",
    "Z_hat = pd.Series(tree_xai.predict(Z), name=\"Label\")\n",
    "pd.concat((Z, Z_hat), axis=1).iloc[:, -1].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon c'est super lent, aussi j'ai codé comme ça me venais un peu, au pire on utilisera l'[implémentation](https://github.com/riccotti/LORE/blob/master/neighbor_generator.py) mais c'est codé differement j'crois\n",
    "\n",
    "Je sais pas c'est quoi les ordre de grandeur des hyprparamètre de l'algo génétique aussi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adresse</th>\n",
       "      <th>Majeur?</th>\n",
       "      <th>Nationalite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Montpellier</td>\n",
       "      <td>oui</td>\n",
       "      <td>Suisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montpellier</td>\n",
       "      <td>oui</td>\n",
       "      <td>Italien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>non</td>\n",
       "      <td>Italien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paris</td>\n",
       "      <td>oui</td>\n",
       "      <td>Suisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>oui</td>\n",
       "      <td>Francais</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adresse Majeur? Nationalite\n",
       "7  Montpellier     oui      Suisse\n",
       "2  Montpellier     oui     Italien\n",
       "4   Strasbourg     non     Italien\n",
       "3        Paris     oui      Suisse\n",
       "6   Strasbourg     oui    Francais"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adresse           Paris\n",
      "Majeur?             oui\n",
      "Nationalite    Francais\n",
      "Name: 0, dtype: object\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "Z = pd.concat((Z_true, Z_false)).reset_index(drop=True)\n",
    "Z_hat = pd.Series(tree_xai.predict(Z), name=\"Label\")\n",
    "# Z, _, Z_hat, _ = train_test_split(\n",
    "#     X, y, test_size=0.01, random_state=42\n",
    "# ) # gros bricolage ici, j'arrive pas à faire passer\n",
    "# Initialiser et entraîner l'arbre de décision symbolique\n",
    "substitution_tree = SymbolicDecisionTree()\n",
    "substitution_tree.fit(Z, Z_hat)\n",
    "\n",
    "sample = X.iloc[0]\n",
    "class_to_not_keep = y.iloc[0]\n",
    "print(sample)\n",
    "print(class_to_not_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfactuals = substitution_tree.find_counterfactuals(class_to_not_keep)\n",
    "n_r_list = substitution_tree.count_rules(sample, counterfactuals)\n",
    "np.argwhere(n_r_list == np.amax(n_r_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Majeur? != non', 'Nationalite == Suisse'], 1), (['Majeur? == non'], 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitution_tree.explain_counterfactuals(sample, class_to_not_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majeur? == non? (Entropy = 0.5000)\n",
      "    ├── Class: -1\n",
      "    └── Nationalite == Suisse? (Entropy = 0.1839)\n",
      "        ├── Class: -1\n",
      "        └── Class: 1\n"
     ]
    }
   ],
   "source": [
    "substitution_tree.display_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
